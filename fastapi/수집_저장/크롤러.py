
"""
ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ ë‹¤ê±´ í¬ë¡¤ëŸ¬ (í˜ì´ì§€ ìˆœíšŒ ë°©ì‹)
pip install requests beautifulsoup4 pymongo
"""

import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient, errors
from datetime import datetime
import json


def fetch_economic_news(limit: int = 50, date: str = None):
    """ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ì—ì„œ ì—¬ëŸ¬ í˜ì´ì§€ ìˆœíšŒí•˜ì—¬ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
    Parameters
    ----------
    limit : int
        ê°€ì ¸ì˜¬ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
    date : str, optional
        YYYYMMDD í˜•ì‹ (ê¸°ë³¸ê°’: ì˜¤ëŠ˜)
    """
    if date is None:
        date = datetime.now().strftime("%Y%m%d")

    headers = {"User-Agent": "Mozilla/5.0"}
    articles = []
    page = 1

    while len(articles) < limit:
        url = (
            f"https://news.naver.com/main/list.naver"
            f"?mode=LSD&mid=shm&sid1=101&date={date}&page={page}"
        )
        res = requests.get(url, headers=headers)
        res.raise_for_status()

        soup = BeautifulSoup(res.text, "html.parser")
        links = [
            a for a in soup.select("ul.type06_headline li dt a")
            if "/article/" in a["href"]
        ]
        if not links:  # ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
            break

        for a in links:
            link = a["href"]
            title = a.get_text(strip=True)
            try:
                art_res = requests.get(link, headers=headers)
                art_res.raise_for_status()
                art_soup = BeautifulSoup(art_res.text, "html.parser")
                content = art_soup.select_one("article#dic_area")
                content = content.get_text(" ", strip=True) if content else "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"
            except Exception as e:
                content = f"ë³¸ë¬¸ ì—ëŸ¬: {e}"

            articles.append({
                "title": title,
                "link": link,
                "content": content,
                "saved_at": datetime.now().isoformat()
            })
            if len(articles) >= limit:
                break
        page += 1

    return articles


def save_to_mongo_or_json(docs: list):
    """MongoDB ì €ì¥ ì‹œë„ â†’ ì‹¤íŒ¨í•˜ë©´ JSON ì €ì¥"""
    try:
        client = MongoClient("mongodb://localhost:27017/", serverSelectionTimeoutMS=2000)
        client.server_info()  # ì—°ê²° í™•ì¸
        db = client["newsdb"]
        col = db["articles"]

        for doc in docs:
            result = col.update_one({"link": doc["link"]}, {"$set": doc}, upsert=True)
            if result.upserted_id:
                print("ğŸ†• MongoDB ìƒˆë¡œ ì €ì¥:", doc["title"])
            elif result.modified_count > 0:
                print("âœï¸ MongoDB ì—…ë°ì´íŠ¸:", doc["title"])
            else:
                print("âš  ë™ì¼ ë°ì´í„° ìˆìŒ:", doc["title"])
    except errors.ServerSelectionTimeoutError:
        print("âš  MongoDB ì‹¤í–‰ ì•ˆë¨ â†’ JSON ì €ì¥í•©ë‹ˆë‹¤.")
        with open("news.json", "w", encoding="utf-8") as f:
            json.dump(docs, f, ensure_ascii=False, indent=2)
        print("âœ… news.json ì €ì¥ ì™„ë£Œ.")


if __name__ == "__main__":
    news_list = fetch_economic_news(limit=50)
    print("ê°€ì ¸ì˜¨ ê¸°ì‚¬ ê°œìˆ˜:", len(news_list))
    for n in news_list:
        print(f"[{n['title']}] {n['link']}")
    save_to_mongo_or_json(news_list)
