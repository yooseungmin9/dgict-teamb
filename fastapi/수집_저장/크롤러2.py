# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ + ìžë™ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ + ê°ì •ë¶„ì„
"""

import re
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List

import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient, ASCENDING

# -------------------
# MongoDB Atlas ì—°ê²° ì •ë³´
# -------------------
MONGODB_URI = "mongodb+srv://Dgict_TeamB:team1234@cluster0.5d0uual.mongodb.net/?retryWrites=true&w=majority"
DB_NAME = "test123"
COLL_NAME = "shared_articles"
USER_AGENT = "Mozilla/5.0"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
)

# -------------------
# í‚¤ì›Œë“œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
# -------------------
CATEGORY_KEYWORDS: Dict[str, List[str]] = {
    "ì¦ê¶Œ": ["ì£¼ì‹", "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì¦ê¶Œ", "ìƒìž¥", "ê±°ëž˜ì†Œ", "ì¦ì‹œ", "ê³µëª¨ì£¼", "IPO", "PER", "ETF"],
    "ê¸ˆìœµ": ["ì€í–‰", "ê¸ˆë¦¬", "ëŒ€ì¶œ", "ì˜ˆê¸ˆ", "ì ê¸ˆ", "ë³´í—˜", "ì¹´ë“œ", "ê¸ˆìœµ", "Fed", "ì—°ì¤€", "ê¸ˆí†µìœ„", "ì±„ê¶Œ"],
    "ë¶€ë™ì‚°": ["ë¶€ë™ì‚°", "ì•„íŒŒíŠ¸", "ì£¼íƒ", "ë¶„ì–‘", "ì „ì„¸", "ë§¤ë§¤", "ì²­ì•½", "ìž¬ê±´ì¶•", "ë¶€ë™ì‚°ì‹œìž¥"],
    "ì‚°ì—…": ["ì‚°ì—…", "ì œì¡°", "ìž¬ê³„", "ê¸°ì—…", "ìžë™ì°¨", "ë°˜ë„ì²´", "ì² ê°•", "ë°”ì´ì˜¤", "IT", "ê³µìž¥", "ì—ë„ˆì§€"],
    "ê¸€ë¡œë²Œê²½ì œ": ["ê¸€ë¡œë²Œ", "ì„¸ê³„", "í•´ì™¸", "êµ­ì œ", "ë¯¸êµ­", "ì¤‘êµ­", "ì¼ë³¸", "ë‹¬ëŸ¬", "ìœ„ì•ˆ", "ìœ ëŸ½"],
    "ì¼ë°˜": []  # ë§¤ì¹­ ì•ˆë˜ë©´ ê¸°ë³¸
}

# ------------------- ê°ì„±ì‚¬ì „
POSITIVE_WORDS = [
    # ê²½ì œ ì „ë°˜ ê¸ì •
    "í˜¸í™©", "ìƒìŠ¹", "ì„±ìž¥", "í˜¸ìž¬", "ê°•ì„¸", "ì´ìµ", "í‘ìž", "ê°œì„ ", "íšŒë³µ",
    "í™•ëŒ€", "ì‹ ê¸°ë¡", "ì•ˆì •", "ê¸ì •", "ë‹¬ì„±", "ëŒíŒŒ",
    # íˆ¬ìž ê´€ë ¨
    "ê¸‰ë“±", "í™œì„±í™”", "ê°•í™”", "í˜¸ì‘", "í˜¸ì¡°", "ìˆ˜ìµ", "ì‹¤ì ê°œì„ ", "ìµœê³ ì¹˜",
    "ì•ˆì •ì„¸", "í˜¸ì „", "ê°€ì†í™”", "ì•½ì§„", "í™•ë³´", "í˜ì‹ ", "ì„ ë„", "ì§„ì¶œ",
    # ì‚¬íšŒì /ì •ì±…ì 
    "ì§€ì›", "íˆ¬ìž", "ê³ ìš©í™•ëŒ€", "í™œí™©", "ë„ì•½", "ì„±ê³µ", "ì„±ê³¼", "ì „ë§ë°ìŒ"
]

NEGATIVE_WORDS = [
    # ê²½ì œ ì „ë°˜ ë¶€ì •
    "ë¶ˆí™©", "í•˜ë½", "ì ìž", "ìœ„ê¸°", "ì•½ì„¸", "ì†ì‹¤", "ì¹¨ì²´", "ì•…í™”", "ìœ„ì¶•",
    "ê¸‰ë½", "ë¶•ê´´", "ë¶ˆì•ˆ", "ë¶€ì •", "íŒŒì‚°", "ë¶€ë„",
    # íˆ¬ìž ê´€ë ¨
    "ê¸‰ê°", "ê²½ê³ ", "ìœ„í˜‘", "ì¶”ë½", "ë§ˆì´ë„ˆìŠ¤", "ì ìží­", "ì €ì¡°", "ê°ì†Œì„¸",
    "ê¸‰ë“±ë½", "ìœ„í—˜", "ë¶ˆí™•ì‹¤", "ë¶€ì§„", "ê¸‰ì¦ì„¸", "ì•½í™”", "ì •ì²´", "ì†Œë©¸",
    # ì‚¬íšŒì /ì •ì±…ì 
    "í•´ê³ ", "êµ¬ì¡°ì¡°ì •", "íì—…", "ë…¼ëž€", "ê°ˆë“±", "ì¹¨ì²´ê¸°", "ì‹¤íŒ¨", "ì°¨ì§ˆ"
]
# -------------------
# MongoDB ìœ í‹¸
# -------------------
def get_collection():
    client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=3000)
    db = client[DB_NAME]
    col = db[COLL_NAME]

    col.create_index([("link", ASCENDING)], name="uniq_link", unique=True)
    col.create_index([("category", ASCENDING), ("news_date", ASCENDING)], name="q_cat_date")
    return col

# -------------------
# í¬ë¡¤ë§ ìœ í‹¸
# -------------------
def build_url(date: str, page: int) -> str:
    base = "https://news.naver.com/main/list.naver"
    return f"{base}?mode=LSD&mid=sec&sid1=101&date={date}&page={page}"

def extract_links(html: str):
    soup = BeautifulSoup(html, "html.parser")
    anchors = soup.select("ul.type06_headline li dt a, ul.type06 li dt a")
    return [(a.get_text(strip=True), a["href"]) for a in anchors if "/article/" in a.get("href", "")]

def fetch_article(link: str, headers: Dict[str, str]) -> str:
    r = requests.get(link, headers=headers, timeout=10)
    r.raise_for_status()
    s = BeautifulSoup(r.text, "html.parser")
    node = s.select_one("article#dic_area")
    return node.get_text(" ", strip=True) if node else "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

# -------------------
# NLP ìœ í‹¸
# -------------------
def classify_category(text: str) -> str:
    for cat, keywords in CATEGORY_KEYWORDS.items():
        for kw in keywords:
            if kw in text:
                return cat
    return "ì¼ë°˜"

def analyze_sentiment(text: str) -> str:
    pos = sum(1 for w in POSITIVE_WORDS if w in text)
    neg = sum(1 for w in NEGATIVE_WORDS if w in text)
    if pos > neg:
        return "ê¸ì •"
    elif neg > pos:
        return "ë¶€ì •"
    else:
        return "ì¤‘ë¦½"

# -------------------
# í¬ë¡¤ëŸ¬ ë³¸ì²´
# -------------------
def crawl_and_save(days: int = 30, limit_per_day: int = 50) -> None:
    col = get_collection()
    headers = {"User-Agent": USER_AGENT}

    for i in range(days):
        date = (datetime.now() - timedelta(days=i)).strftime("%Y%m%d")
        logging.info("ðŸ“… %s | ê²½ì œ ì „ì²´", date)

        page, inserted, skipped = 1, 0, 0

        while inserted < limit_per_day:
            url = build_url(date, page)
            res = requests.get(url, headers=headers, timeout=10)
            res.raise_for_status()
            links = extract_links(res.text)
            if not links:
                break

            for title, link in links:
                if col.find_one({"link": link}):
                    skipped += 1
                    continue

                try:
                    content = fetch_article(link, headers)
                except Exception as e:
                    content = f"ë³¸ë¬¸ ì—ëŸ¬: {e}"

                # ì¹´í…Œê³ ë¦¬/ê°ì • ë¶„ì„
                category = classify_category(title + " " + content)
                sentiment = analyze_sentiment(content)

                doc = {
                    "title": title,
                    "link": link,
                    "content": content,
                    "category": category,
                    "sentiment": sentiment,
                    "news_date": date,
                    "saved_at": datetime.now().isoformat(),
                }

                col.insert_one(doc)
                inserted += 1

                if inserted >= limit_per_day:
                    break

            page += 1
            time.sleep(0.2)

        logging.info("   â–¶ ì €ìž¥ ê²°ê³¼ | insert=%d, skip=%d", inserted, skipped)

    logging.info("âœ… ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ")

# -------------------
# ì‹¤í–‰ë¶€
# -------------------
if __name__ == "__main__":
    crawl_and_save(days=30, limit_per_day=50)
