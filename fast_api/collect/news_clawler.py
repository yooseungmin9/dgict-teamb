# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (ê²½ì œ ë‰´ìŠ¤ ë‚ ì§œë³„ 50ê°œ ì €ì¥)
- category: í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
- ìš”ì•½ + ì¶œì²˜ + ê°ì„±ë¶„ì„ + í‚¤ì›Œë“œì¶”ì¶œ + DBì €ì¥
"""

import re, time, logging
from datetime import datetime, timedelta
from typing import Dict, List

import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient, ASCENDING
from transformers import pipeline
from konlpy.tag import Okt
from dateutil import parser  # ë‚ ì§œ íŒŒì‹±

# -------------------
# MongoDB Atlas ì—°ê²° ì •ë³´
# -------------------
MONGO_URI = "mongodb+srv://Dgict_TeamB:team1234@cluster0.5d0uual.mongodb.net/"
DB_NAME   = "test123"
COLL_NAME = "shared_articles"

# -------------------
# UA & ì–¸ë¡ ì‚¬ ë§¤í•‘
# -------------------
UA = {"User-Agent": "Mozilla/5.0", "Referer": "https://news.naver.com/"}
BASE_LIST = "https://news.naver.com/main/list.naver"
OIDS = {
  "056": "KBS",
  "015": "í•œêµ­ê²½ì œ",
  "009": "ë§¤ì¼ê²½ì œ",
  "014": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
  "119": "ë°ì¼ë¦¬ì•ˆ",
  "005": "êµ­ë¯¼ì¼ë³´",
  "421": "ë‰´ìŠ¤1",
  "047": "ì˜¤ë§ˆì´ë‰´ìŠ¤",
  "001": "ì—°í•©ë‰´ìŠ¤",
  "629": "ë”íŒ©íŠ¸",
  "029": "ë””ì§€í„¸íƒ€ì„ìŠ¤",
  "008": "ë¨¸ë‹ˆíˆ¬ë°ì´",
  "028": "í•œê²¨ë ˆ",
  "448": "TVì¡°ì„ ",
  "023": "ì¡°ì„ ì¼ë³´",
  "082": "ë¶€ì‚°ì¼ë³´",
  "277": "ì•„ì‹œì•„ê²½ì œ",
  "422": "ì—°í•©ë‰´ìŠ¤TV",
  "018": "ì´ë°ì¼ë¦¬",
  "092": "ì§€ë””ë„·ì½”ë¦¬ì•„",
  "052": "YTN",
  "020": "ë™ì•„ì¼ë³´",
  "055": "SBS",
  "003": "ë‰´ì‹œìŠ¤",
  "469": "í•œêµ­ì¼ë³´",
  "366": "ì¡°ì„ ë¹„ì¦ˆ",
  "025": "ì¤‘ì•™ì¼ë³´",
  "079": "ë…¸ì»·ë‰´ìŠ¤",
  "659": "ì „ì£¼MBC",
  "437": "JTBC",
  "016": "í—¤ëŸ´ë“œê²½ì œ",
  "032": "ê²½í–¥ì‹ ë¬¸",
  "214": "MBC",
  "215": "í•œêµ­ê²½ì œTV",
  "138": "ë””ì§€í„¸ë°ì¼ë¦¬",
  "011": "ì„œìš¸ê²½ì œ",
  "586": "ì‹œì‚¬ì €ë„",
  "044": "ì½”ë¦¬ì•„í—¤ëŸ´ë“œ",
  "002": "í”„ë ˆì‹œì•ˆ",
  "021": "ë¬¸í™”ì¼ë³´",
  "087": "ê°•ì›ì¼ë³´",
  "081": "ì„œìš¸ì‹ ë¬¸",
  "666": "ê²½ê¸°ì¼ë³´",
  "088": "ë§¤ì¼ì‹ ë¬¸",
  "057": "MBN",
  "449": "ì±„ë„A",
  "022": "ì„¸ê³„ì¼ë³´",
  "374": "SBS Biz",
  "030": "ì „ìì‹ ë¬¸",
  "346": "í—¬ìŠ¤ì¡°ì„ ",
  "037": "ì£¼ê°„ë™ì•„",
  "656": "ëŒ€ì „ì¼ë³´",
  "031": "ì•„ì´ë‰´ìŠ¤24",
  "648": "ë¹„ì¦ˆì›Œì¹˜",
  "660": "kbcê´‘ì£¼ë°©ì†¡",
  "640": "ì½”ë¦¬ì•„ì¤‘ì•™ë°ì¼ë¦¬",
  "654": "ê°•ì›ë„ë¯¼ì¼ë³´",
  "607": "ë‰´ìŠ¤íƒ€íŒŒ",
  "661": "JIBS",
  "006": "ë¯¸ë””ì–´ì˜¤ëŠ˜",
  "310": "ì—¬ì„±ì‹ ë¬¸",
  "262": "ì‹ ë™ì•„",
  "094": "ì›”ê°„ ì‚°",
  "308": "ì‹œì‚¬IN",
  "024": "ë§¤ê²½ì´ì½”ë…¸ë¯¸",
  "293": "ë¸”ë¡œí„°",
  "123": "ì¡°ì„¸ì¼ë³´",
  "657": "ëŒ€êµ¬MBC",
  "662": "ë†ë¯¼ì‹ ë¬¸",
  "243": "ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸",
  "417": "ë¨¸ë‹ˆS",
  "036": "í•œê²¨ë ˆ21",
  "584": "ë™ì•„ì‚¬ì´ì–¸ìŠ¤",
  "007": "ì¼ë‹¤",
  "050": "í•œê²½ë¹„ì¦ˆë‹ˆìŠ¤",
  "655": "CJBì²­ì£¼ë°©ì†¡",
  "033": "ì£¼ê°„ê²½í–¥",
  "296": "ì½”ë©”ë””ë‹·ì»´",
  "053": "ì£¼ê°„ì¡°ì„ ",
  "127": "ê¸°ìí˜‘íšŒë³´",
  "658": "êµ­ì œì‹ ë¬¸",
  "665": "ë”ìŠ¤ì¿ í”„",
  "353": "ì¤‘ì•™SUNDAY",
  "145": "ë ˆì´ë””ê²½í–¥"
}

# -------------------
# ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ (í…ìŠ¤íŠ¸ ê¸°ë°˜)
# -------------------
CATEGORY_KEYWORDS_WEIGHT: Dict[str, Dict[str, float]] = {
    # --------------------- #
    # ğŸ“ˆ ì¦ê¶Œ
    # --------------------- #
    "ì¦ê¶Œ": {
        "ì¦ì‹œ": 3.0, "ì£¼ì‹": 3.0, "ì½”ìŠ¤í”¼": 2.8, "ì½”ìŠ¤ë‹¥": 2.8, "ìƒì¥": 2.2,
        "IPO": 2.0, "ETF": 2.0, "ë¦¬ì¸ ": 2.0, "ê±°ë˜ì†Œ": 2.0, "ì‹œê°€ì´ì•¡": 2.0,
        "ë§¤ìˆ˜": 1.8, "ë§¤ë„": 1.8, "ì™¸êµ­ì¸": 1.8, "ê¸°ê´€íˆ¬ìì": 1.8,
        "ì„ ë¬¼": 1.8, "ì˜µì…˜": 1.8, "ê³µë§¤ë„": 1.6, "ìœ ë™ì„±": 1.5, "PER": 1.5, "PBR": 1.5,
        "í…Œë§ˆì£¼": 1.4, "í˜¸ì¬": 1.2, "ì•…ì¬": 1.2, "ë°°ë‹¹": 1.2, "ë¦¬ë°¸ëŸ°ì‹±": 1.0,
        "ì¦ê¶Œì‚¬": 1.5, "íˆ¬ìì": 1.5, "í¬íŠ¸í´ë¦¬ì˜¤": 1.3, "ì‹œí™©": 1.2, "ì£¼ê°€": 2.0
    },

    # --------------------- #
    # ğŸ’° ê¸ˆìœµ
    # --------------------- #
    "ê¸ˆìœµ": {
        "ê¸ˆìœµ": 3.0, "ì€í–‰": 2.8, "ê¸ˆë¦¬": 3.0, "ëŒ€ì¶œ": 2.5, "ì˜ˆê¸ˆ": 2.3, "ë³´í—˜": 2.3,
        "ì±„ê¶Œ": 2.3, "ì¹´ë“œ": 2.0, "ì—°ì¤€": 2.8, "í•œêµ­ì€í–‰": 2.5, "í™˜ìœ¨": 2.3,
        "í†µí™”ì •ì±…": 2.3, "ê¸°ì¤€ê¸ˆë¦¬": 2.5, "ì—¬ì‹ ": 2.0, "ìˆ˜ì‹ ": 1.8,
        "í•€í…Œí¬": 2.0, "P2P": 1.5, "ë¨¸ë‹ˆë§ˆì¼“": 1.8, "ë¦¬ìŠ¤í¬ê´€ë¦¬": 1.8,
        "ë¶€ì±„": 1.8, "ë¶€ì‹¤ì±„ê¶Œ": 1.8, "ì‹ ìš©ë“±ê¸‰": 1.5, "CB": 1.5, "íšŒì‚¬ì±„": 1.5,
        "ì•”í˜¸í™”í": 2.0, "ë¹„íŠ¸ì½”ì¸": 2.0, "ì´ë”ë¦¬ì›€": 1.8, "CBDC": 1.5, "STO": 1.5,
        "ê¸ˆìœµì‹œì¥": 2.3, "ê¸ˆìœµìœ„": 1.8, "ê¸ˆìœµê°ë…ì›": 1.8, "ì˜ˆëŒ€ê¸ˆë¦¬ì°¨": 1.5,
        "ê¸´ì¶•": 2.0, "ì–‘ì ì™„í™”": 1.8, "ìœ ë™ì„±ìœ„ê¸°": 2.2
    },

    # --------------------- #
    # ğŸ  ë¶€ë™ì‚°
    # --------------------- #
    "ë¶€ë™ì‚°": {
        "ë¶€ë™ì‚°": 3.0, "ì•„íŒŒíŠ¸": 3.0, "ì£¼íƒ": 2.8, "ì „ì„¸": 2.5, "ë§¤ë§¤": 2.5,
        "ì²­ì•½": 2.3, "ë¶„ì–‘": 2.3, "ì¬ê±´ì¶•": 2.3, "ì¬ê°œë°œ": 2.3, "PF": 3.0,
        "ë¶€ë™ì‚°PF": 3.0, "í”„ë¡œì íŠ¸íŒŒì´ë‚¸ì‹±": 2.5, "ê±´ì„¤ì‚¬": 2.0, "ê³µì‹œì§€ê°€": 2.0,
        "ì§‘ê°’": 2.5, "ì‹œì„¸": 2.0, "ë§¤ë§¤ê°€": 2.0, "ì „ì„¸ê°€": 2.0, "ì„ëŒ€ì°¨": 1.8,
        "ë¦¬ëª¨ë¸ë§": 1.5, "ë¶€ë™ì‚°íˆ¬ì": 1.8, "ë¦¬ì¸ ": 1.8, "ëŒ€ì¶œê·œì œ": 1.8,
        "ì „ì„¸ì‚¬ê¸°": 2.0, "ë¯¸ë¶„ì–‘": 2.0, "ì…ì£¼": 1.5, "ê±°ë˜ì ˆë²½": 1.5,
        "ì¢…ë¶€ì„¸": 1.8, "ì¬ì‚°ì„¸": 1.8, "ë„ì‹œê³„íš": 1.5, "ìƒí™œê¶Œ": 1.2,
        "ê°•ë‚¨êµ¬": 1.5, "ì„œì´ˆêµ¬": 1.5, "ì†¡íŒŒêµ¬": 1.5, "ë¶„ë‹¹": 1.2, "ì¼ì‚°": 1.2,
        "ë¶€ë™ì‚°ì‹œì¥": 2.0, "ì „ì›”ì„¸": 1.8, "ë¶€ë™ì‚°ëŒ€ì¶œ": 2.0, "ì£¼ê±°ë³µì§€": 1.2
    },

    # --------------------- #
    # âš™ï¸ ì‚°ì—…
    # --------------------- #
    "ì‚°ì—…": {
        "ì‚°ì—…": 3.0, "ê¸°ì—…": 2.8, "ì œì¡°": 2.5, "ë°˜ë„ì²´": 3.0, "ë°°í„°ë¦¬": 2.8,
        "ìë™ì°¨": 2.8, "ì „ê¸°ì°¨": 2.5, "ìˆ˜ì†Œì°¨": 2.0, "ì¡°ì„ ": 2.0, "í•­ê³µ": 2.0,
        "ë¬¼ë¥˜": 2.0, "ìœ í†µ": 2.0, "ì¤‘ê³µì—…": 2.0, "í™”í•™": 2.0, "ì² ê°•": 2.0,
        "ì—ë„ˆì§€": 2.3, "í”ŒëœíŠ¸": 1.8, "ë¡œë´‡": 2.3, "AI": 2.5, "í´ë¼ìš°ë“œ": 2.3,
        "5G": 1.8, "6G": 1.8, "ë””ì§€í„¸ì „í™˜": 2.3, "ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬": 2.0,
        "ë¦¬íŠ¬ì´ì˜¨": 2.0, "ì´ì°¨ì „ì§€": 2.0, "ì–‘ê·¹ì¬": 1.8, "ìŒê·¹ì¬": 1.8,
        "ì†Œì¬": 1.8, "R&D": 1.8, "ì‚°ë‹¨": 1.5, "ê·¸ë¦°ì‚°ì—…": 1.5, "íƒ„ì†Œì¤‘ë¦½": 1.8,
        "ê³µê¸‰ë§": 2.3, "AIë°˜ë„ì²´": 3.0, "ì²¨ë‹¨ì‚°ì—…": 2.5, "í˜ì‹ ": 1.8
    },

    # --------------------- #
    # ğŸŒ ê¸€ë¡œë²Œê²½ì œ
    # --------------------- #
    "ê¸€ë¡œë²Œê²½ì œ": {
        "ê¸€ë¡œë²Œ": 3.0, "êµ­ì œ": 2.8, "í•´ì™¸": 2.5, "ì„¸ê³„": 2.3, "ë¯¸êµ­": 3.0, "ì¤‘êµ­": 3.0,
        "ì¼ë³¸": 2.5, "ìœ ëŸ½": 2.5, "ë‹¬ëŸ¬": 2.3, "ì—”í™”": 2.0, "ìœ„ì•ˆí™”": 2.0,
        "WTI": 2.3, "ë¸Œë ŒíŠ¸ìœ ": 2.3, "ìœ ê°€": 2.3, "IMF": 2.3, "OECD": 2.3, "WTO": 2.0,
        "FOMC": 2.5, "ECB": 2.3, "BOJ": 2.0, "PBOC": 2.0,
        "ë‚˜ìŠ¤ë‹¥": 2.3, "ë‹¤ìš°": 2.3, "S&P500": 2.3,
        "ë¬´ì—­": 2.3, "ìˆ˜ì¶œ": 2.3, "ìˆ˜ì…": 2.3, "ê´€ì„¸": 2.0, "ë¸Œë¦­ìŠ¤": 2.0, "ì‹ í¥êµ­": 2.0,
        "ê¸€ë¡œë²Œê²½ê¸°": 2.5, "í™˜ìœ¨ì „ìŸ": 2.0, "ë¦¬ìŠ¤í¬ì˜¨": 1.5, "ë¦¬ìŠ¤í¬ì˜¤í”„": 1.5,
        "OPEC": 1.8, "OPEC+": 1.8, "ê³µê¸‰ë§ìœ„ê¸°": 1.8, "ì§€ì •í•™": 1.8, "ë¯¸ì¤‘ê°ˆë“±": 2.3,
        "G20": 1.5, "ìœ ë¡œì¡´": 1.5, "êµ­ì œìœ ê°€": 2.3, "ë‹¬ëŸ¬ê°•ì„¸": 2.0
    },

    # --------------------- #
    # ğŸ§¾ ì¼ë°˜ê²½ì œ
    # --------------------- #
    "ì¼ë°˜": {
        "ê²½ì œ": 3.0, "ë¬¼ê°€": 2.8, "ì†Œë¹„": 2.5, "ê²½ê¸°": 2.5, "ì„±ì¥ë¥ ": 2.3,
        "GDP": 2.3, "ê³ ìš©": 2.3, "ì‹¤ì—…": 2.0, "ë…¸ë™": 2.0, "ì„ê¸ˆ": 2.0,
        "ìƒí™œ": 2.0, "ì†Œë“": 2.0, "ì§€ì¶œ": 2.0, "ê°€ê³„ë¶€ì±„": 2.3,
        "ì°½ì—…": 2.0, "ìì˜ì—…": 2.0, "ì†Œìƒê³µì¸": 2.0, "í”„ëœì°¨ì´ì¦ˆ": 1.8,
        "ë‚´ìˆ˜": 2.0, "ë¬¼ê°€ìƒìŠ¹": 2.3, "ê²½ê¸°ì¹¨ì²´": 2.3, "ê²½ê¸°íšŒë³µ": 2.0,
        "ì²´ê°ê²½ê¸°": 1.8, "ìƒí™œë¹„": 1.8, "ë¬¼ê°€ìƒìŠ¹ë¥ ": 1.8,
        "ì €ì¶œì‚°": 1.5, "ê³ ë ¹í™”": 1.5, "ë³µì§€": 1.5, "ì„œë¯¼ê²½ì œ": 1.8,
        "ì†Œë¹„ì‹¬ë¦¬": 1.8, "ë¦¬ì‡¼ì–´ë§": 1.2, "ê³ ìš©ë¥ ": 1.5, "ì‹¤ì—…ë¥ ": 1.5
    }
}
POSITIVE_WORDS = [
    # ê²½ê¸°/ì‹œì¥
    "í˜¸í™©","ìƒìŠ¹","ì„±ì¥","í˜¸ì¬","ê°•ì„¸","í˜¸ì „","ì´ìµ","í‘ì","ê°œì„ ","íšŒë³µ","í™•ëŒ€","ì‹ ê¸°ë¡",
    "ì•ˆì •","ê¸ì •","ë‹¬ì„±","ëŒíŒŒ","í™œí™©","í™œì„±í™”","í˜¸ì¡°","ì•½ì§„","ê°•í™”","í˜¸í‰","ë‚™ê´€",
    "í’ë¶€","í’ë…„","ë„ì•½","í˜ì‹ ","ìµœê³ ì¹˜","ê³ ê³µí–‰ì§„","ê¸°ëŒ€ê°","í›ˆí’","ê²¬ì¡°","íƒ„íƒ„",
    # ì •ì±…/ê¸ˆìœµ
    "ê¸ˆë¦¬ì¸í•˜","ê°ì„¸","ì§€ì›","íˆ¬ìí™•ëŒ€","ì •ì±…íš¨ê³¼","ìœ ì…","ìˆœì´ìµ","ë°°ë‹¹","í˜¸ì‹¤ì ","ìê¸ˆíë¦„",
    "ê³ ìš©ì¦ê°€","ì†Œë“ì¦ê°€","ë§¤ì¶œí˜¸ì¡°","í‘ìì „í™˜","ì¦ê°€ì„¸","ìƒí–¥ì¡°ì •","ìˆ˜ìµì„±ê°œì„ ","ì‹œì¥í™•ëŒ€",
    # ê¸€ë¡œë²Œ/ì‚°ì—…
    "ìˆ˜ì¶œí˜¸ì¡°","ìˆ˜ì¶œì¦ê°€","ë¬´ì—­í‘ì","ê¸€ë¡œë²Œí˜¸í™©","í˜‘ë ¥ê°•í™”","ìœ ì¹˜ì„±ê³µ","í•©ì˜","í˜‘ìƒíƒ€ê²°",
    "ê¸°ìˆ í˜ì‹ ","ëŒíŒŒêµ¬","ì•ˆì°©","ì„±ê³µ","ì„±ê³¼","í˜¸ì‘","ì‹ ê·œíˆ¬ì","ì„±ì¥ë™ë ¥"
]

NEGATIVE_WORDS = [
    # ê²½ê¸°/ì‹œì¥
    "ë¶ˆí™©","í•˜ë½","ì ì","ìœ„ê¸°","ì•½ì„¸","ì†ì‹¤","ì¹¨ì²´","ì•…í™”","ìœ„ì¶•","ê¸‰ë½","ë¶•ê´´","ë¶ˆì•ˆ",
    "ë¶€ì •","íŒŒì‚°","ë¶€ë„","í­ë½","í‡´ë³´","í•˜í–¥","ë¶€ì§„","ì¹¨í•˜","ì—­ì„±ì¥","ì—­í’","ì¶©ê²©","ë¦¬ìŠ¤í¬",
    "ì¹¨ì²´êµ­ë©´","í•˜ë°©ì••ë ¥","ë‘”í™”","ìœ„ê¸°ê°","ê²½ìƒ‰","ì•…ì¬","ê²½ì°©ë¥™","ë¶ˆíˆ¬ëª…","ì¹¨ì²´ìš°ë ¤",
    # ì •ì±…/ê¸ˆìœµ
    "ê¸ˆë¦¬ì¸ìƒ","ê¸´ì¶•","ë¶€ë‹´","ë¶€ì±„","ì ìì „í™˜","ì±„ë¬´ë¶ˆì´í–‰","ì‹ ìš©ìœ„ê¸°","íŒŒì‚°ì‹ ì²­","ì—°ì²´",
    "êµ¬ì¡°ì¡°ì •","ë§¤ì¶œê°ì†Œ","ìˆœì†ì‹¤","ë§ˆì´ë„ˆìŠ¤","ë¶€ë„ìœ„ê¸°","íˆ¬ììœ„ì¶•","íˆ¬ìê°ì†Œ",
    # ê¸€ë¡œë²Œ/ì‚°ì—…
    "ìˆ˜ì¶œê°ì†Œ","ë¬´ì—­ì ì","ê´€ì„¸ë¶€ê³¼","ë¬´ì—­ë¶„ìŸ","ë¶ˆí™•ì‹¤","ì°¨ì§ˆ","ê³ ìš©ê°ì†Œ","ì‹¤ì—…","íì—…",
    "ì² ìˆ˜","ê°ì‚°","íŒŒì—…","ë…¸ì‚¬ê°ˆë“±","ì†Œì†¡","ê·œì œ","ë¶ˆë§¤ìš´ë™","ì œì¬","ì¹¨ì²´ì‹¬í™”","ìœ ì¶œ",
    "ëŒ€ëŸ‰í•´ê³ ","ì ìëˆ„ì ","íƒ€ê²©","ë¶ˆí˜‘í™”ìŒ","ê°ˆë“±","ë¶„ìŸ"
]
# -------------------
# Summarizer ì„¤ì •
# -------------------
MODEL_ID   = "EbanLee/kobart-summary-v3"
summarizer = pipeline("summarization", model=MODEL_ID, tokenizer=MODEL_ID, device=-1)
MAX_LEN, MIN_LEN, NUM_BEAMS, DO_SAMPLE, PRE_CUT = 400, 50, 6, False, 1500

def preprocess_txt(text: str) -> str:
    return text.replace("\n"," ").replace("\t"," ").strip()[:PRE_CUT] if text else ""

def summarize_text(text: str) -> str:
    clean = preprocess_txt(text)
    if not clean: return ""
    try:
        out = summarizer(clean, max_length=MAX_LEN, min_length=MIN_LEN,
                         num_beams=NUM_BEAMS, do_sample=DO_SAMPLE)
        return out[0]["summary_text"].strip()
    except Exception:
        return ""

# -------------------
# ì¹´í…Œê³ ë¦¬ & ê°ì„±ë¶„ì„
# -------------------
def classify_category_weighted(title: str, content: str) -> str:
    """
    ì œëª©(title) + ë³¸ë¬¸(content)ì„ ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìˆ˜ë¥¼ ê³„ì‚°
    - ì œëª©ì€ 1.5ë°° ê°€ì¤‘ì¹˜
    - CATEGORY_KEYWORDS_WEIGHT ë”•ì…”ë„ˆë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ì‚°
    """
    scores = {}

    for cat, keywords in CATEGORY_KEYWORDS_WEIGHT.items():
        score = 0
        for kw, w in keywords.items():
            count = title.count(kw) * 1.5 + content.count(kw)
            score += count * w
        scores[cat] = score

    best_cat = max(scores, key=scores.get)
    return best_cat if scores[best_cat] > 0 else "ì¼ë°˜"

def sentiment_score(text: str) -> int:
    score = sum(1 for w in POSITIVE_WORDS if w in text)
    score -= sum(1 for w in NEGATIVE_WORDS if w in text)
    return score

# -------------------
# í‚¤ì›Œë“œ ì¶”ì¶œ
# -------------------
from pathlib import Path
from typing import Set

okt = Okt()
TOKEN_RE = re.compile(r"[ê°€-í£A-Za-z0-9]{2,}")

from typing import Optional, Set

def load_stopwords(path: str = "stopwords.txt", extra: Optional[Set[str]] = None) -> set:
    """
    í•œêµ­ì–´ ë‰´ìŠ¤ìš© ë¶ˆìš©ì–´ ë¡œë” (Python 3.9~3.10 í˜¸í™˜).
    - íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¸íŠ¸ë¥¼ ì‚¬ìš©.
    - extra ì¸ìë¡œ ì¶”ê°€ ë‹¨ì–´ë¥¼ í•©ì¹¨.
    """
    base = {
        # ì¡°ì‚¬Â·ì ‘ì†ì‚¬
        "ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼","ì—","ì—ì„œ","ìœ¼ë¡œ","ë¡œ","ê³¼","ì™€","ì˜","ë„","ë§Œ",
        "ë³´ë‹¤","ê¹Œì§€","í–ˆë‹¤","ìˆë‹¤","ë˜ë‹¤","í•˜ë‹¤","ìœ„í•´","í†µí•´","ëŒ€í•œ","ë°","ë“±","ì¤‘","ê²ƒ","ìˆ˜","ë•Œë¬¸","ê´€ë ¨","ëŒ€í•´",
        # ì‹œì Â·ë¹ˆì¶œ ì„œìˆ 
        "ì˜¤ëŠ˜","ì–´ì œ","ë‚´ì¼","í˜„ì¬","ìµœê·¼","ë‹¹ì‹œ","ì‘ë…„","ì˜¬í•´","ì´ë²ˆ","ì§€ë‚œ","ê¸ˆì£¼","ì „ì¼","ì „ë‚ ","ê¸°ì","ì‚¬ì§„","ì˜ìƒ","ì†ë³´"
    }

    try:
        with open(path, encoding="utf-8") as f:
            base |= {w.strip().lower() for w in f if w.strip()}
    except FileNotFoundError:
        pass

    if extra:
        base |= {w.lower() for w in extra}

    return base

# ì–¸ë¡ ì‚¬ëª…(OIDS ê°’)ì„ ë¶ˆìš©ì–´ì— ìë™ í¬í•¨
PRESS_WORDS = {v for v in OIDS.values()}  # ì˜ˆ: {"ì—°í•©ë‰´ìŠ¤","í•œêµ­ê²½ì œ",...}
STOPWORDS = load_stopwords(extra=PRESS_WORDS)

def extract_keywords(text: str) -> List[str]:
    """
    í˜•íƒœì†Œ ë¶„ì„ â†’ ëª…ì‚¬/ì•ŒíŒŒë²³ â†’ ì •ê·œì‹ í•„í„° â†’ ë¶ˆìš©ì–´ ì œê±° â†’ ì¤‘ë³µ ì œê±°
    ì´ˆë³´ íŒ: 'stopwords.txt'ë¥¼ ê°™ì€ ë””ë ‰í„°ë¦¬ì— ë‘ê³  ë‹¨ì–´ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì¶”ê°€í•˜ì„¸ìš”.
    """
    if not text:
        return []
    pos = okt.pos(text, norm=True, stem=True)  # ì •ê·œí™”+ì–´ê°„í™”
    candidates = (w for (w, p) in pos if p in ("Noun", "Alpha"))

    tokens, seen = [], set()
    for w in candidates:
        w = w.lower()
        if not TOKEN_RE.fullmatch(w):      # 2ì ì´ìƒ í•œ/ì˜/ìˆ«ìë§Œ
            continue
        if w in STOPWORDS:                  # ë¶ˆìš©ì–´ ì œê±°
            continue
        if len(w) < 2:                      # ì•ˆì „ ê¸¸ì´ í•„í„°
            continue
        if w not in seen:
            tokens.append(w)
            seen.add(w)
    return tokens
# -------------------
# MongoDB
# -------------------
def get_collection():
    client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=3000)
    col = client[DB_NAME][COLL_NAME]
    col.create_index([("url", ASCENDING)], name="uniq_url", unique=True)
    return col

# -------------------
# í¬ë¡¤ë§
# -------------------
def build_url(date: str, page: int) -> str:
    return f"{BASE_LIST}?mode=LSD&mid=sec&sid1=101&date={date}&page={page}"

def extract_links(html: str):
    soup = BeautifulSoup(html, "html.parser")
    anchors = soup.select("ul.type06_headline li dt a, ul.type06 li dt a")
    return [(a.get_text(strip=True), a["href"]) for a in anchors if "/article/" in a.get("href", "")]

def fetch_article(link: str) -> Dict[str, str]:
    r = requests.get(link, headers=UA, timeout=10)
    r.raise_for_status()
    s = BeautifulSoup(r.text, "html.parser")

    title_node = s.select_one("h2#title_area, h3#articleTitle, meta[property='og:title']")
    title = title_node.get("content") if title_node and title_node.name=="meta" else (title_node.get_text(strip=True) if title_node else "")

    body_node = s.select_one("article#dic_area, div#articeBody, div#newsct_article")
    content = body_node.get_text(" ", strip=True) if body_node else ""

    og = s.select_one('meta[property="og:image"]')
    image_url = og["content"] if og and og.get("content") else ""

    press = ""
    m = re.search(r"article/(\d{3})/", link)
    if m and m.group(1) in OIDS:
        press = OIDS[m.group(1)]

    pub_time = None
    t1 = s.select_one("span.media_end_head_info_datestamp_time")
    if t1 and t1.get("data-date-time"):
        pub_time = parser.parse(t1["data-date-time"])
    else:
        t2 = s.select_one('meta[property="og:article:published_time"]')
        if t2 and t2.get("content"):
            pub_time = parser.parse(t2["content"])

    return {"title": title, "content": content, "image": image_url, "press": press, "published_at": pub_time}

# -------------------
# ì‹¤í–‰ ë³¸ì²´
# -------------------
def crawl_and_save(days: int = 1, limit_per_day: int = 100):
    col = get_collection()
    counters = {}

    for i in range(days):
        date = (datetime.now() - timedelta(days=i)).strftime("%Y%m%d")
        logging.info("ğŸ“… ìˆ˜ì§‘ ì‹œì‘: %s", date)
        inserted, page = 0, 1

        while inserted < limit_per_day:
            url = build_url(date, page)
            res = requests.get(url, headers=UA, timeout=10)
            res.raise_for_status()
            links = extract_links(res.text)
            if not links: break

            for title, link in links:
                if col.find_one({"url": link}): continue
                art = fetch_article(link)
                if not art["published_at"]: continue

                pub_date = art["published_at"].date()
                if pub_date.strftime("%Y%m%d") != date: continue
                if counters.get(pub_date, 0) >= limit_per_day: continue

                summary = summarize_text(art["content"])
                senti = sentiment_score(art["title"] + " " + art["content"])
                kws = extract_keywords(art["title"] + " " + art["content"])
                cat = classify_category_weighted(art["title"], art["content"])


                doc = {
                    "title": art["title"] or title,
                    "url": link,
                    "content": art["content"],
                    "image": art["image"],
                    "summary": summary,
                    "press": art["press"],
                    "main_section": "ê²½ì œ",
                    "category": cat,
                    "sentiment_score": senti,
                    "keywords": kws,
                    "published_at": art["published_at"].isoformat(),  # ê¸°ì‚¬ ë°œí–‰ì¼ ê¸°ì¤€
                }
                try:
                    col.insert_one(doc)
                    counters[pub_date] = counters.get(pub_date, 0) + 1
                    inserted += 1
                    logging.info("[OK] ì €ì¥ë¨: %s (%s)", link, pub_date)
                except Exception as e:
                    logging.warning("[SKIP] ì €ì¥ ì‹¤íŒ¨: %s", e)

                if inserted >= limit_per_day: break

            page += 1
            time.sleep(0.3)

        logging.info("â–¶ %s ë‚ ì§œ ê²°ê³¼: %dê°œ", date, inserted)

    logging.info("âœ… ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ (ì¼ë³„ ì¹´ìš´íŠ¸: %s)", counters)

if __name__ == "__main__":
    crawl_and_save(days=1, limit_per_day=100)
