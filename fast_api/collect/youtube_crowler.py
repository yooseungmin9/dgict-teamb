# -*- coding: utf-8 -*-
"""
ìœ íŠœë¸Œ API + ëŒ“ê¸€ + ë‹¤ì¤‘ ê°ì •ë¶„ì„(7í´ë˜ìŠ¤, ë°°ì¹˜ ì²˜ë¦¬) â†’ MongoDB ì‹¤ì‹œê°„ ì €ì¥
"""

from googleapiclient.discovery import build
from pymongo import MongoClient
import googleapiclient.errors
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch

# --------------------------
# 1. API & DB ì„¤ì •
# --------------------------
API_KEY = "AIzaSyAvBT58ksxCS_E0nehgiy5fMJbtXnePghk"  # ğŸ”‘ ë³¸ì¸ í‚¤
youtube = build("youtube", "v3", developerKey=API_KEY, static_discovery=False)

client = MongoClient("mongodb+srv://Dgict_TeamB:team1234@cluster0.5d0uual.mongodb.net/")
db = client["test123"]
col = db["youtube_db2"]

# --------------------------
# 2. í•œêµ­ì–´ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ
# --------------------------
MODEL_NAME = "dlckdfuf141/korean-emotion-kluebert-v2"

id2label = {
    0: "ê³µí¬(Fear)",
    1: "ë†€ëŒ(Surprise)",
    2: "ë¶„ë…¸(Anger)",
    3: "ìŠ¬í””(Sadness)",
    4: "ì¤‘ë¦½(Neutral)",
    5: "í–‰ë³µ(Happiness)",
    6: "í˜ì˜¤(Disgust)"
}
label2id = {v: k for k, v in id2label.items()}

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=7,
    id2label=id2label,
    label2id=label2id
)

emotion_classifier = pipeline(
    "text-classification",
    model=model,
    tokenizer=tokenizer,
    top_k=None,
    batch_size=32,  # ğŸ”‘ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¶”ë¡ 
    device=0 if torch.cuda.is_available() else -1
)

# --------------------------
# 3. ë°°ì¹˜ ê°ì • ë¶„ì„ í•¨ìˆ˜
# --------------------------
def classify_emotions_batch(texts):
    """ì—¬ëŸ¬ ëŒ“ê¸€ì„ í•œ ë²ˆì— ê°ì • ë¶„ë¥˜"""
    preds = emotion_classifier(
        texts, truncation=True, max_length=256
    )
    results = []
    for pred in preds:
        scores = {p["label"]: float(p["score"]) for p in pred}
        best = max(scores, key=scores.get)
        results.append({"label": best, "scores": scores})
    return results

# --------------------------
# 4. ì˜ìƒ ì •ë³´
# --------------------------
def get_video_info(video_id: str, category: str):
    request = youtube.videos().list(part="snippet,statistics", id=video_id)
    response = request.execute()
    if not response["items"]:
        return None

    item = response["items"][0]
    snippet = item["snippet"]
    stats = item["statistics"]

    return {
        "video_id": video_id,
        "category": category,
        "title": snippet["title"],
        "published_at": snippet["publishedAt"],
        "view_count": int(stats.get("viewCount", 0)),
        "comment_count": int(stats.get("commentCount", 0)),
        "thumbnail_url": snippet["thumbnails"]["medium"]["url"],
    }

# --------------------------
# 5. ëŒ“ê¸€ ìˆ˜ì§‘ + ë°°ì¹˜ ê°ì • ë¶„ì„
# --------------------------
def get_video_comments(video_id: str, max_total: int = 1000):
    comments = []
    next_page_token = None

    try:
        while len(comments) < max_total:
            request = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                maxResults=100,
                pageToken=next_page_token,
                textFormat="plainText"
            )
            response = request.execute()

            batch_texts = []
            raw_items = []

            for item in response.get("items", []):
                snippet = item["snippet"]["topLevelComment"]["snippet"]
                text = snippet.get("textDisplay", "")
                batch_texts.append(text)
                raw_items.append(snippet)

                if len(comments) + len(batch_texts) >= max_total:
                    break

            # ğŸ”‘ ë°°ì¹˜ë¡œ ê°ì •ë¶„ì„
            if batch_texts:
                emotions = classify_emotions_batch(batch_texts)
                for snip, emo in zip(raw_items, emotions):
                    comments.append({
                        "author": snip.get("authorDisplayName"),
                        "text": snip.get("textDisplay", ""),
                        "published_at": snip.get("publishedAt"),
                        "like_count": snip.get("likeCount", 0),
                        "emotion": emo["label"],
                        "emotion_scores": emo["scores"]
                    })

            next_page_token = response.get("nextPageToken")
            if not next_page_token:
                break

    except googleapiclient.errors.HttpError as e:
        if "commentsDisabled" in str(e):
            print(f"âš ï¸ ëŒ“ê¸€ êº¼ì§ â†’ ìˆ˜ì§‘ ì•ˆ í•¨ (video_id={video_id})")
            return None

    return comments

# --------------------------
# 6. ë©”ì¸ ìˆ˜ì§‘ (ì¤‘ë³µÂ·ë¬´ê´€ í•„í„°ë§ ì¶”ê°€)
# --------------------------
from difflib import SequenceMatcher

def is_similar(a: str, b: str, threshold: float = 0.8) -> bool:
    """ë¬¸ìì—´ ìœ ì‚¬ë„ ê³„ì‚°"""
    return SequenceMatcher(None, a, b).ratio() > threshold

def collect_videos_with_emotions(query: str, category: str, target_count: int = 10):
    saved_count = 0
    next_page_token = None
    seen_titles = []  # ì¤‘ë³µ ì œëª© ê°ì‹œ

    while saved_count < target_count:
        request = youtube.search().list(
            part="snippet",
            q=query,
            type="video",
            order="relevance",
            maxResults=50,
            pageToken=next_page_token
        )
        response = request.execute()

        for item in response["items"]:
            if saved_count >= target_count:
                break

            video_id = item["id"]["videoId"]

            # (1) DB ì¤‘ë³µ í™•ì¸
            if col.find_one({"video_id": video_id}):
                print(f"âš ï¸ ì¤‘ë³µ ì˜ìƒ â†’ ìŠ¤í‚µ ({video_id})")
                continue

            info = get_video_info(video_id, category)
            if not info:
                continue

            title = info["title"].strip()
            desc = item["snippet"].get("description", "")

            # (3) ì œëª© ìœ ì‚¬ë„ ê²€ì‚¬
            if any(is_similar(title, t) for t in seen_titles):
                print(f"âš ï¸ ìœ ì‚¬ ì œëª© â†’ ìŠ¤í‚µ: {title}")
                continue

            # (4) ëŒ“ê¸€ ìˆ˜ ê¸°ì¤€
            if info["comment_count"] < 20:
                print(f"âŒ ëŒ“ê¸€ {info['comment_count']}ê°œ â†’ ìŠ¤í‚µ: {title}")
                continue

            comments = get_video_comments(video_id, max_total=1000)
            if not comments:
                continue

            info["comments"] = comments
            col.insert_one(info)
            saved_count += 1
            seen_titles.append(title)
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {title} ({len(comments)}ê°œ ëŒ“ê¸€)")

        next_page_token = response.get("nextPageToken")
        if not next_page_token:
            print("âš ï¸ ë” ì´ìƒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            break

    print(f"ğŸ“Š ìµœì¢… ì €ì¥ëœ ì˜ìƒ ìˆ˜: {saved_count}")

# --------------------------
# 7. ì‹¤í–‰
# --------------------------
if __name__ == "__main__":
    query = "ì‚°ì—… ë‰´ìŠ¤"
    category = "ì‚°ì—…"
    collect_videos_with_emotions(query, category, target_count=20)  # âœ… target_count ì‚¬ìš©

