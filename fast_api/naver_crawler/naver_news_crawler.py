# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ + ìš”ì•½ + ì¹´í…Œê³ ë¦¬ + ì¶œì²˜ + ê°ì„±ë¶„ì„ + í‚¤ì›Œë“œì¶”ì¶œ + DBì €ìž¥
"""

import re, time, logging
from datetime import datetime, timedelta
from typing import Dict, List

import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient, ASCENDING
from transformers import pipeline
from konlpy.tag import Okt   # âœ… í‚¤ì›Œë“œ ì¶”ì¶œìš©

# -------------------
# MongoDB Atlas ì—°ê²° ì •ë³´
# -------------------
MONGO_URI = "mongodb+srv://Dgict_TeamB:team1234@cluster0.5d0uual.mongodb.net/"
DB_NAME   = "test123"
COLL_NAME = "shared_articles"

# -------------------
# UA & ì–¸ë¡ ì‚¬ ë§¤í•‘
# -------------------
UA = {"User-Agent": "Mozilla/5.0", "Referer": "https://news.naver.com/"}
BASE_LIST = "https://news.naver.com/main/list.naver"
OIDS = {
  "056": "KBS",
  "015": "í•œêµ­ê²½ì œ",
  "009": "ë§¤ì¼ê²½ì œ",
  "014": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
  "119": "ë°ì¼ë¦¬ì•ˆ",
  "005": "êµ­ë¯¼ì¼ë³´",
  "421": "ë‰´ìŠ¤1",
  "047": "ì˜¤ë§ˆì´ë‰´ìŠ¤",
  "001": "ì—°í•©ë‰´ìŠ¤",
  "629": "ë”íŒ©íŠ¸",
  "029": "ë””ì§€í„¸íƒ€ìž„ìŠ¤",
  "008": "ë¨¸ë‹ˆíˆ¬ë°ì´",
  "028": "í•œê²¨ë ˆ",
  "448": "TVì¡°ì„ ",
  "023": "ì¡°ì„ ì¼ë³´",
  "082": "ë¶€ì‚°ì¼ë³´",
  "277": "ì•„ì‹œì•„ê²½ì œ",
  "422": "ì—°í•©ë‰´ìŠ¤TV",
  "018": "ì´ë°ì¼ë¦¬",
  "092": "ì§€ë””ë„·ì½”ë¦¬ì•„",
  "052": "YTN",
  "020": "ë™ì•„ì¼ë³´",
  "055": "SBS",
  "003": "ë‰´ì‹œìŠ¤",
  "469": "í•œêµ­ì¼ë³´",
  "366": "ì¡°ì„ ë¹„ì¦ˆ",
  "025": "ì¤‘ì•™ì¼ë³´",
  "079": "ë…¸ì»·ë‰´ìŠ¤",
  "659": "ì „ì£¼MBC",
  "437": "JTBC",
  "016": "í—¤ëŸ´ë“œê²½ì œ",
  "032": "ê²½í–¥ì‹ ë¬¸",
  "214": "MBC",
  "215": "í•œêµ­ê²½ì œTV",
  "138": "ë””ì§€í„¸ë°ì¼ë¦¬",
  "011": "ì„œìš¸ê²½ì œ",
  "586": "ì‹œì‚¬ì €ë„",
  "044": "ì½”ë¦¬ì•„í—¤ëŸ´ë“œ",
  "002": "í”„ë ˆì‹œì•ˆ",
  "021": "ë¬¸í™”ì¼ë³´",
  "087": "ê°•ì›ì¼ë³´",
  "081": "ì„œìš¸ì‹ ë¬¸",
  "666": "ê²½ê¸°ì¼ë³´",
  "088": "ë§¤ì¼ì‹ ë¬¸",
  "057": "MBN",
  "449": "ì±„ë„A",
  "022": "ì„¸ê³„ì¼ë³´",
  "374": "SBS Biz",
  "030": "ì „ìžì‹ ë¬¸",
  "346": "í—¬ìŠ¤ì¡°ì„ ",
  "037": "ì£¼ê°„ë™ì•„",
  "656": "ëŒ€ì „ì¼ë³´",
  "031": "ì•„ì´ë‰´ìŠ¤24",
  "648": "ë¹„ì¦ˆì›Œì¹˜",
  "660": "kbcê´‘ì£¼ë°©ì†¡",
  "640": "ì½”ë¦¬ì•„ì¤‘ì•™ë°ì¼ë¦¬",
  "654": "ê°•ì›ë„ë¯¼ì¼ë³´",
  "607": "ë‰´ìŠ¤íƒ€íŒŒ",
  "661": "JIBS",
  "006": "ë¯¸ë””ì–´ì˜¤ëŠ˜",
  "310": "ì—¬ì„±ì‹ ë¬¸",
  "262": "ì‹ ë™ì•„",
  "094": "ì›”ê°„ ì‚°",
  "308": "ì‹œì‚¬IN",
  "024": "ë§¤ê²½ì´ì½”ë…¸ë¯¸",
  "293": "ë¸”ë¡œí„°",
  "123": "ì¡°ì„¸ì¼ë³´",
  "657": "ëŒ€êµ¬MBC",
  "662": "ë†ë¯¼ì‹ ë¬¸",
  "243": "ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸",
  "417": "ë¨¸ë‹ˆS",
  "036": "í•œê²¨ë ˆ21",
  "584": "ë™ì•„ì‚¬ì´ì–¸ìŠ¤",
  "007": "ì¼ë‹¤",
  "050": "í•œê²½ë¹„ì¦ˆë‹ˆìŠ¤",
  "655": "CJBì²­ì£¼ë°©ì†¡",
  "033": "ì£¼ê°„ê²½í–¥",
  "296": "ì½”ë©”ë””ë‹·ì»´",
  "053": "ì£¼ê°„ì¡°ì„ ",
  "127": "ê¸°ìží˜‘íšŒë³´",
  "658": "êµ­ì œì‹ ë¬¸",
  "665": "ë”ìŠ¤ì¿ í”„",
  "353": "ì¤‘ì•™SUNDAY",
  "145": "ë ˆì´ë””ê²½í–¥"
}

# -------------------
# ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ
# -------------------
CATEGORY_KEYWORDS: Dict[str, List[str]] = {
    "ì¦ê¶Œ": ["ì£¼ì‹","ì½”ìŠ¤í”¼","ì½”ìŠ¤ë‹¥","ì¦ê¶Œ","ìƒìž¥","ê±°ëž˜ì†Œ","ì¦ì‹œ","ê³µëª¨ì£¼","IPO","PER","ETF"],
    "ê¸ˆìœµ": ["ì€í–‰","ê¸ˆë¦¬","ëŒ€ì¶œ","ì˜ˆê¸ˆ","ì ê¸ˆ","ë³´í—˜","ì¹´ë“œ","ê¸ˆìœµ","Fed","ì—°ì¤€","ê¸ˆí†µìœ„","ì±„ê¶Œ"],
    "ë¶€ë™ì‚°": ["ë¶€ë™ì‚°","ì•„íŒŒíŠ¸","ì£¼íƒ","ë¶„ì–‘","ì „ì„¸","ë§¤ë§¤","ì²­ì•½","ìž¬ê±´ì¶•","ë¶€ë™ì‚°ì‹œìž¥"],
    "ì‚°ì—…": ["ì‚°ì—…","ì œì¡°","ìž¬ê³„","ê¸°ì—…","ìžë™ì°¨","ë°˜ë„ì²´","ì² ê°•","ë°”ì´ì˜¤","IT","ê³µìž¥","ì—ë„ˆì§€"],
    "ê¸€ë¡œë²Œê²½ì œ": ["ê¸€ë¡œë²Œ","ì„¸ê³„","í•´ì™¸","êµ­ì œ","ë¯¸êµ­","ì¤‘êµ­","ì¼ë³¸","ë‹¬ëŸ¬","ìœ„ì•ˆ","ìœ ëŸ½"],
    "ì¼ë°˜": []
}

POSITIVE_WORDS = ["í˜¸í™©","ìƒìŠ¹","ì„±ìž¥","í˜¸ìž¬","ê°•ì„¸","ì´ìµ","í‘ìž","ê°œì„ ","íšŒë³µ","í™•ëŒ€","ì‹ ê¸°ë¡","ì•ˆì •","ê¸ì •","ë‹¬ì„±","ëŒíŒŒ"]
NEGATIVE_WORDS = ["ë¶ˆí™©","í•˜ë½","ì ìž","ìœ„ê¸°","ì•½ì„¸","ì†ì‹¤","ì¹¨ì²´","ì•…í™”","ìœ„ì¶•","ê¸‰ë½","ë¶•ê´´","ë¶ˆì•ˆ","ë¶€ì •","íŒŒì‚°","ë¶€ë„"]

# -------------------
# Summarizer ì„¤ì •
# -------------------
MODEL_ID   = "EbanLee/kobart-summary-v3"
summarizer = pipeline("summarization", model=MODEL_ID, tokenizer=MODEL_ID, device=-1)

MAX_LEN, MIN_LEN, NUM_BEAMS, DO_SAMPLE, PRE_CUT = 400, 50, 6, False, 1500

def preprocess_txt(text: str) -> str:
    if not text: return ""
    text = text.replace("\n", " ").replace("\t", " ").strip()
    return text[:PRE_CUT]

def summarize_text(text: str) -> str:
    clean = preprocess_txt(text)
    if not clean: return ""
    try:
        out = summarizer(clean, max_length=MAX_LEN, min_length=MIN_LEN,
                         num_beams=NUM_BEAMS, do_sample=DO_SAMPLE)
        return out[0]["summary_text"].strip()
    except Exception as e:
        logging.warning("ìš”ì•½ ì‹¤íŒ¨: %s", e)
        return ""

# -------------------
# ì¹´í…Œê³ ë¦¬ & ê°ì„±ë¶„ì„
# -------------------
def classify_category(text: str) -> str:
    for cat, keywords in CATEGORY_KEYWORDS.items():
        if any(kw in text for kw in keywords):
            return cat
    return "ì¼ë°˜"

def sentiment_score(text: str) -> int:
    score = sum(1 for w in POSITIVE_WORDS if w in text)
    score -= sum(1 for w in NEGATIVE_WORDS if w in text)
    return score

# -------------------
# í‚¤ì›Œë“œ ì¶”ì¶œ (Okt ì‚¬ìš©)
# -------------------
okt = Okt()
STOPWORDS = {"ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼","ì—","ì—ì„œ","ìœ¼ë¡œ","ë¡œ","ê³¼","ì™€","ì˜","ë„","ë§Œ","ë³´ë‹¤","ê¹Œì§€","í–ˆë‹¤","ìžˆë‹¤"}
TOKEN_RE = re.compile(r"[ê°€-íž£A-Za-z0-9]{2,}")

def extract_keywords(text: str) -> List[str]:
    pos = okt.pos(text, norm=True, stem=True)
    cand = [w for (w,p) in pos if p in ("Noun","Alpha")]
    tokens, seen = [], set()
    for w in cand:
        w = w.lower()
        if not TOKEN_RE.fullmatch(w): continue
        if w in STOPWORDS: continue
        if w not in seen:
            tokens.append(w); seen.add(w)
    return tokens

# -------------------
# MongoDB
# -------------------
def get_collection():
    client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=3000)
    col = client[DB_NAME][COLL_NAME]
    col.create_index([("url", ASCENDING)], name="uniq_url", unique=True)
    return col

# -------------------
# í¬ë¡¤ë§
# -------------------
def build_url(date: str, page: int) -> str:
    return f"{BASE_LIST}?mode=LSD&mid=sec&sid1=101&date={date}&page={page}"

def extract_links(html: str):
    soup = BeautifulSoup(html, "html.parser")
    anchors = soup.select("ul.type06_headline li dt a, ul.type06 li dt a")
    return [(a.get_text(strip=True), a["href"]) for a in anchors if "/article/" in a.get("href", "")]

def fetch_article(link: str) -> Dict[str, str]:
    r = requests.get(link, headers=UA, timeout=10)
    r.raise_for_status()
    s = BeautifulSoup(r.text, "html.parser")

    title_node = s.select_one("h2#title_area, h3#articleTitle, meta[property='og:title']")
    title = title_node.get("content") if title_node and title_node.name=="meta" else (title_node.get_text(strip=True) if title_node else "")

    body_node = s.select_one("article#dic_area, div#articeBody, div#newsct_article")
    content = body_node.get_text(" ", strip=True) if body_node else ""

    og = s.select_one('meta[property="og:image"]')
    image_url = og["content"] if og and og.get("content") else ""

    # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
    press = ""
    m = re.search(r"article/(\d{3})/", link)
    if m and m.group(1) in OIDS:
        press = OIDS[m.group(1)]

    return {"title": title, "content": content, "image": image_url, "press": press}

# -------------------
# ì‹¤í–‰ ë³¸ì²´
# -------------------
def crawl_and_save(days: int = 30, limit_per_day: int = 50):
    col = get_collection()

    for i in range(days):
        date = (datetime.now() - timedelta(days=i)).strftime("%Y%m%d")
        logging.info("ðŸ“… %s | ê²½ì œ ì „ì²´", date)

        page, inserted = 1, 0
        while inserted < limit_per_day:
            url = build_url(date, page)
            res = requests.get(url, headers=UA, timeout=10)
            res.raise_for_status()
            links = extract_links(res.text)
            if not links: break

            for title, link in links:
                if col.find_one({"url": link}): continue
                try:
                    art = fetch_article(link)
                except Exception as e:
                    logging.warning("ë³¸ë¬¸ ì—ëŸ¬: %s", e)
                    continue

                # ìš”ì•½, ì¹´í…Œê³ ë¦¬, ê°ì„±, í‚¤ì›Œë“œ
                summary = summarize_text(art["content"])
                cat = classify_category(art["title"] + " " + art["content"])
                senti = sentiment_score(art["title"] + " " + art["content"])
                kws = extract_keywords(art["title"] + " " + art["content"])

                doc = {
                    "title": art["title"] or title,
                    "url": link,
                    "content": art["content"],
                    "image": art["image"],
                    "summary": summary,
                    "press": art["press"],
                    "category": cat,
                    "sentiment_score": senti,
                    "keywords": kws,
                    "published_at": datetime.now().isoformat(),
                }
                try:
                    col.insert_one(doc)
                    logging.info("[OK] ì €ìž¥ë¨: %s", link)
                    inserted += 1
                except Exception as e:
                    logging.warning("[SKIP] ì €ìž¥ ì‹¤íŒ¨: %s", e)

                if inserted >= limit_per_day: break

            page += 1
            time.sleep(0.3)

        logging.info("â–¶ ì €ìž¥ ê²°ê³¼ | insert=%d", inserted)

    logging.info("âœ… ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ")

if __name__ == "__main__":
    crawl_and_save(days=30, limit_per_day=50)
