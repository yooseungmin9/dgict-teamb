# -*- coding: utf-8 -*-
# FastAPI + Aladin API â†’ (í•„í„°/ì¤‘ë³µì œê±°) â†’ [ì˜µì…˜] MongoDB ì €ì¥(save=true)
# ì‹¤í–‰:
#   pip install fastapi uvicorn pymongo requests
#   uvicorn main:app --reload --host 0.0.0.0 --port 8000
# ì‚¬ìš© ì˜ˆ:
#   GET /books?pages=2&per_page=50&save=true
# ì—…ë°ì´íŠ¸ìš©
#   GET /_refresh_bestseller_meta?pages=3&per_page=20

#   GET /_ping_write  â† Mongo ì“°ê¸° í™•ì¸ìš©
#   POST /_repair_isbn_index  â† isbn13:null ì •ë¦¬ + ì¸ë±ìŠ¤ ì¬ìƒì„±

import requests
from typing import List, Dict, Optional
from fastapi import FastAPI, Query, HTTPException
from datetime import datetime
from xml.etree import ElementTree as ET

from pymongo import MongoClient, UpdateOne, ASCENDING, DESCENDING, TEXT
from pymongo.errors import DuplicateKeyError, BulkWriteError

# =========================
# ğŸ”§ ê³ ì • ì„¤ì •(ì—¬ê¸°ë§Œ ìˆ˜ì •)
# =========================
# 1) ì•Œë¼ë”˜ TTB í‚¤
TTBKEY = "ttb2win0min1217001"  # â† ë„¤ í‚¤ë¡œ êµì²´

# 2) MongoDB ì—°ê²° ë° DB/ì»¬ë ‰ì…˜ ì´ë¦„
MONGODB_URI = "mongodb+srv://Dgict_TeamB:team1234@cluster0.5d0uual.mongodb.net/"
MONGODB_DB  = "test123"

# ğŸ‘‰ ì»¬ë ‰ì…˜ ì´ë¦„ì€ ì—¬ê¸°ì„œ í•œ ë²ˆë§Œ ì§€ì •
BOOKS_COLLECTION      = "aladin_books"
CATEGORIES_COLLECTION = "aladin_categories"

# 3) ê¸°ë³¸ ê²½ì œ ì¹´í…Œê³ ë¦¬(í•„ìš”ì‹œ ìˆ˜ì •/ì¶”ê°€)
DEFAULT_ECON_CATEGORY_IDS = [
    170, 3057, 3059, 3061, 3062, 3063, 8586, 3065, 3140, 8587,
    2172, 3103, 2173, 2841, 8593, 2747, 3123, 3069, 2028, 853,
    852, 854, 261, 268, 273, 1632, 55058, 2169, 263, 172, 141092,
    11502, 175, 11501, 2225, 174, 177, 3048, 32288, 180, 249,
    3049, 3104, 2408, 3110, 11503, 6189
]

# 4) ì•Œë¼ë”˜ API ê¸°ë³¸
ALADIN_URL = "http://www.aladin.co.kr/ttb/api/ItemList.aspx"

app = FastAPI(title="Aladin Save-First API (Mongo, rank+salesPoint)")

# =========================
# ê³µí†µ ìœ í‹¸
# =========================
def normalize_category_ids(raw_ids: Optional[List[str]]) -> List[int]:
    """'3065,3057' ì´ë“  ['3065','3057'] ì´ë“  ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ê·œí™”"""
    if not raw_ids:
        return []
    out: List[int] = []
    for v in raw_ids:
        if v is None:
            continue
        for part in str(v).split(","):
            p = part.strip()
            if p.isdigit():
                out.append(int(p))
    return list(dict.fromkeys(out))  # ì¤‘ë³µ ì œê±°(ìˆœì„œ ë³´ì¡´)

# =========================
# Mongo ìœ í‹¸
# =========================
def get_db():
    client = MongoClient(MONGODB_URI)
    return client[MONGODB_DB]

def drop_index_if_exists(coll, name: str):
    try:
        coll.drop_index(name)
        print(f"[ensure_indexes] dropped index: {name}")
    except Exception:
        # ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ë¬´ì‹œ
        pass

def ensure_indexes(db):
    # ì¹´í…Œê³ ë¦¬
    db[CATEGORIES_COLLECTION].create_index([("id", ASCENDING)], unique=True, name="uk_category_id")
    db[CATEGORIES_COLLECTION].create_index([("name", ASCENDING)], name="idx_category_name")

    # ë„ì„œ (ê³ ìœ í‚¤/ISBN/ë§í¬/ì¹´í…Œê³ ë¦¬+ì¶œê°„ì¼/ì¶œê°„ì¼/í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤)
    db[BOOKS_COLLECTION].create_index([("uniqueKey", ASCENDING)], unique=True, sparse=True, name="uk_uniquekey")

    # isbn13 unique+sparse (Noneì€ ì¸ë±ìŠ¤ ì œì™¸)
    drop_index_if_exists(db[BOOKS_COLLECTION], "uk_isbn13")
    db[BOOKS_COLLECTION].create_index([("isbn13", ASCENDING)], unique=True, sparse=True, name="uk_isbn13")

    db[BOOKS_COLLECTION].create_index([("link", ASCENDING)], unique=True, sparse=True, name="uk_link")
    db[BOOKS_COLLECTION].create_index([("categoryId", ASCENDING), ("pubDate", ASCENDING)], name="idx_cat_pubDate")
    db[BOOKS_COLLECTION].create_index([("pubDate", ASCENDING)], name="idx_pubDate")
    db[BOOKS_COLLECTION].create_index([("title", TEXT), ("author", TEXT)], name="txt_title_author")

    # âœ… ì¸ê¸° ì •ë ¬ìš© ì¸ë±ìŠ¤ ì¶”ê°€
    db[BOOKS_COLLECTION].create_index([("salesPoint", DESCENDING)], name="idx_salesPoint")
    db[BOOKS_COLLECTION].create_index([("bestseller.rank", ASCENDING), ("bestseller.categoryId", ASCENDING)], name="idx_bsr_cat")

# =========================
# ìœ ì§€ë³´ìˆ˜/ì§„ë‹¨ ì—”ë“œí¬ì¸íŠ¸
# =========================
@app.get("/_ping_write")
def ping_write():
    """MongoDB ì—°ê²°/ì“°ê¸° í™•ì¸ìš© ê°„ë‹¨ í…ŒìŠ¤íŠ¸"""
    try:
        db = get_db()
        ensure_indexes(db)
        now = datetime.utcnow()
        doc = {
            "uniqueKey": f"_ping_{now.timestamp()}",
            "title": "PING",
            "author": "SYSTEM",
            "pubDate": now,
            "categoryId": 0,
            "source": "TEST",
            "ingestedAt": now,
            "updatedAt": now,
            # í…ŒìŠ¤íŠ¸ìš© í•„ë“œ (ì—†ì–´ë„ ë¨)
            "salesPoint": 1,
            "bestseller": {"rank": 999999, "categoryId": 0, "capturedAt": now}
        }
        db[BOOKS_COLLECTION].insert_one(doc)
        return {"ok": True, "collection": BOOKS_COLLECTION, "db": MONGODB_DB}
    except Exception as e:
        return {"ok": False, "error": str(e)}

@app.post("/_repair_isbn_index")
def repair_isbn_index():
    """
    1) isbn13:null ë¬¸ì„œ ì •ë¦¬(unset)
    2) uk_isbn13 ì¸ë±ìŠ¤ ì¬ìƒì„±(sparse unique)
    """
    db = get_db()
    # 1) isbn13:null â†’ í•„ë“œ ì œê±°
    res = db[BOOKS_COLLECTION].update_many({"isbn13": None}, {"$unset": {"isbn13": ""}})
    # 2) ì¸ë±ìŠ¤ ì¬ìƒì„±
    drop_index_if_exists(db[BOOKS_COLLECTION], "uk_isbn13")
    db[BOOKS_COLLECTION].create_index([("isbn13", ASCENDING)], unique=True, sparse=True, name="uk_isbn13")
    return {"ok": True, "unset_null_count": res.modified_count, "index": "uk_isbn13 (unique,sparse)"}

# =========================
# Aladin í˜¸ì¶œ & íŒŒì‹±
# =========================
def fetch_xml(cid: int, start: int, max_results: int) -> str:
    """Bestseller ë¦¬ìŠ¤íŠ¸ í˜¸ì¶œ (QueryType=Bestseller)"""
    if not TTBKEY:
        raise HTTPException(500, "ALADIN_TTBKEY ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    params = {
        "ttbkey": TTBKEY,
        "QueryType": "Bestseller",
        "MaxResults": max_results,
        "start": start,
        "SearchTarget": "Book",
        "CategoryId": cid,
        "output": "xml",
        "Version": 20131101,
    }
    r = requests.get(ALADIN_URL, params=params, timeout=10)
    r.raise_for_status()
    return r.text

def parse_xml(xml_text: str) -> List[Dict]:
    """
    XML íŒŒì‹±: title/author/pubDate/link/cover/isbn13 + salesPoint(ìˆì„ ê²½ìš°) + í˜ì´ì§€ ë‚´ ì¸ë±ìŠ¤
    """
    try:
        root = ET.fromstring(xml_text)
    except ET.ParseError:
        return []

    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì œê±°
    for el in root.iter():
        if isinstance(el.tag, str) and '}' in el.tag:
            el.tag = el.tag.split('}', 1)[1]

    out = []
    for idx, it in enumerate(root.iter("item")):
        def get(tag):
            n = it.find(tag)
            return (n.text or "").strip() if n is not None else ""

        # salesPointëŠ” ì—†ì„ ìˆ˜ë„ ìˆìŒ â†’ ì•ˆì „ íŒŒì‹±
        sp_raw = get("salesPoint")
        try:
            sales_point = int(sp_raw) if sp_raw else None
        except ValueError:
            sales_point = None

        out.append({
            "title":  get("title"),
            "author": get("author"),
            "pubDate": get("pubDate")[:10],  # YYYY-MM-DD
            "link":   get("link"),
            "cover":  get("cover"),
            "isbn13": get("isbn13"),
            "salesPoint": sales_point,       # âœ… íŒë§¤ ì§€ìˆ˜(ìˆìœ¼ë©´)
            "__index_in_page": idx           # âœ… í˜ì´ì§€ ë‚´ ìˆœì„œ (ì ˆëŒ€ ë­í¬ ê³„ì‚°ì— ì‚¬ìš©)
        })
    return out

# =========================
# Mongo ì €ì¥(ì—…ì„œíŠ¸) + ì§„ë‹¨ ë¡œê·¸
# =========================
def save_to_mongo(items: List[Dict], category_ids: List[int]):
    print(f"[save_to_mongo] incoming items: {len(items)}  cats={category_ids}")
    db = get_db()
    ensure_indexes(db)

    # ì¹´í…Œê³ ë¦¬ upsert
    for cid in set(category_ids):
        db[CATEGORIES_COLLECTION].update_one(
            {"id": int(cid)},
            {"$set": {"id": int(cid), "name": "ECON"},
             "$setOnInsert": {"createdAt": datetime.utcnow()}},
            upsert=True,
        )

    ops = []
    now = datetime.utcnow()
    for it in items:
        raw_isbn = (it.get("isbn13") or "").strip()
        isbn = raw_isbn if raw_isbn else None
        link = (it.get("link") or "").strip() or None
        unique_key = isbn or link
        if not unique_key:
            # isbnë„ linkë„ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            continue

        pub_dt = None
        if it.get("pubDate"):
            try:
                pub_dt = datetime.strptime(it["pubDate"], "%Y-%m-%d")
            except Exception:
                pub_dt = None

        # category_idsê°€ ë‹¨ì¼ì´ë©´ ê·¸ ê°’ì„ categoryIdë¡œ, ë³µìˆ˜ë©´ ì•„ì´í…œì— ì‹¬ì–´ë†“ì€ ê°’ ì‚¬ìš©
        cat_id = category_ids[0] if len(category_ids) == 1 else it.get("categoryId")

        set_fields = {
            "title": it.get("title") or "",
            "author": it.get("author") or "",
            "cover": it.get("cover") or None,
            "pubDate": pub_dt,
            "categoryId": int(cat_id) if cat_id else None,
            "source": "ALADIN",
            "link": link,
            "updatedAt": now,
        }
        if isbn is not None:
            set_fields["isbn13"] = isbn

        # âœ… íŒë§¤ ì§€ìˆ˜ ì €ì¥(ìˆì„ ë•Œë§Œ)
        if it.get("salesPoint") is not None:
            try:
                set_fields["salesPoint"] = int(it["salesPoint"])
            except Exception:
                pass

        # âœ… ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì ˆëŒ€ ë­í¬ ì €ì¥(ìˆì„ ë•Œë§Œ)
        if it.get("bestsellerRank") is not None:
            set_fields["bestseller"] = {
                "rank": int(it["bestsellerRank"]),
                "categoryId": int(cat_id) if cat_id else None,
                "capturedAt": now  # ì–¸ì œì˜ ë­í¬ì¸ì§€ ìŠ¤ëƒ…ìƒ· ì‹œê°„
            }

        update_doc = {"$set": set_fields, "$setOnInsert": {"ingestedAt": now, "uniqueKey": unique_key}}
        # í˜¹ì‹œ ì´ì „ì— isbn13:null ì´ ì €ì¥ëœ ì ì´ ìˆë‹¤ë©´ ì œê±°
        if isbn is None:
            update_doc["$unset"] = {"isbn13": ""}

        ops.append(UpdateOne({"uniqueKey": unique_key}, update_doc, upsert=True))

    if not ops:
        print("[save_to_mongo] no ops to write (missing isbn13/link?)")
        return

    try:
        result = db[BOOKS_COLLECTION].bulk_write(ops, ordered=False)
        print(f"[save_to_mongo] bulk_write OK. upserted={result.upserted_count} matched={result.matched_count} modified={result.modified_count}")
    except DuplicateKeyError as e:
        print(f"[save_to_mongo] DuplicateKeyError: {e}")
    except BulkWriteError as bwe:
        # ì¼ë¶€ ë¬¸ì„œ ì‹¤íŒ¨ ì‹œì—ë„ ì„±ê³µë¶„ì€ ë°˜ì˜ë¨. ì„¸ë¶€ ì‚¬ìœ ë§Œ ë¡œê·¸
        print(f"[save_to_mongo] BulkWriteError: {bwe.details}")
    except Exception as e:
        print(f"[save_to_mongo] bulk_write error: {e}")
        raise

# =========================
# API ì—”ë“œí¬ì¸íŠ¸
# =========================
@app.get("/books")
def books(
    # ì½¤ë§ˆ/ë°˜ë³µ í—ˆìš©
    category_ids: Optional[List[str]] = Query(
        None, description="ì˜ˆ: 3065,3057 ë˜ëŠ” category_ids=3065&category_ids=3057"
    ),
    start: int  = Query(1, ge=1),
    pages: int  = Query(1, ge=1, le=5),
    per_page: int = Query(20, ge=1, le=50),
    since: Optional[str] = None,  # YYYY-MM-DD
    save: bool = Query(False, description="trueë©´ MongoDBì— ì €ì¥ í›„ ë°˜í™˜"),
):
    # 1) ì¹´í…Œê³ ë¦¬ ì •ê·œí™”
    ids = normalize_category_ids(category_ids)
    if not ids:
        ids = DEFAULT_ECON_CATEGORY_IDS
    if not ids:
        raise HTTPException(400, "category_idsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    # 2) ìˆ˜ì§‘
    collected: List[Dict] = []
    for cid in ids:
        s = start
        for _ in range(pages):
            try:
                xml = fetch_xml(cid, s, per_page)
                parsed = parse_xml(xml)

                # âœ… ì ˆëŒ€ ë­í¬ ê³„ì‚°: Bestseller í˜¸ì¶œ ì‹œ ì‘ë‹µ ìˆœì„œë¥¼ ë­í¬ë¡œ ì‚¬ìš©
                base = (s - 1) * per_page  # ì´ì „ í˜ì´ì§€ê¹Œì§€ì˜ ê°œìˆ˜
                for it in parsed:
                    it.setdefault("categoryId", cid)
                    local_idx = it.pop("__index_in_page", None)
                    if local_idx is not None:
                        it["bestsellerRank"] = base + local_idx + 1  # 1-based

                collected.extend(parsed)
            except requests.RequestException as e:
                print(f"[books] request error (cid={cid}, page={s}): {e}")
            s += 1

    print(f"[books] collected={len(collected)} before dedup")

    # 3) ì¤‘ë³µ ì œê±° (isbn13 ìš°ì„ , ì—†ìœ¼ë©´ link)
    seen = set()
    dedup = []
    for it in collected:
        key = it.get("isbn13") or it.get("link")
        if not key:
            continue
        if key in seen:
            continue
        seen.add(key)
        dedup.append(it)

    print(f"[books] after dedup={len(dedup)}")

    # 4) since í•„í„° (ì„ íƒ)
    if since:
        try:
            since_dt = datetime.strptime(since[:10], "%Y-%m-%d")
            filtered = []
            for it in dedup:
                pub = (it.get("pubDate") or "")[:10]
                try:
                    if datetime.strptime(pub, "%Y-%m-%d") >= since_dt:
                        filtered.append(it)
                except Exception:
                    filtered.append(it)
            dedup = filtered
            print(f"[books] after since filter={len(dedup)} (since={since})")
        except ValueError:
            print(f"[books] invalid since format, ignored: {since}")

    # 5) save=trueì´ë©´ Mongoì— ì—…ì„œíŠ¸
    if save:
        try:
            save_to_mongo(dedup, ids)
        except Exception as e:
            raise HTTPException(500, f"Mongo ì €ì¥ ì‹¤íŒ¨: {e}")

    return {"count": len(dedup), "items": dedup, "saved": bool(save)}

def update_sales_meta(items: List[dict], category_ids: List[int]):
    """
    ê¸°ì¡´ save_to_mongoëŠ” ì „ì²´ ë¬¸ì„œë¥¼ ì—…ì„œíŠ¸.
    ì´ í•¨ìˆ˜ëŠ” ë©”íƒ€(salesPoint, bestseller, updatedAt)ë§Œ 'ë¶€ë¶„ ì—…ë°ì´íŠ¸' í•©ë‹ˆë‹¤.
    """
    from pymongo import UpdateOne
    db = get_db()
    ensure_indexes(db)

    ops = []
    now = datetime.utcnow()
    for it in items:
        key = (it.get("isbn13") or "").strip() or (it.get("link") or "").strip()
        if not key:
            continue

        set_fields = {"updatedAt": now}

        if it.get("salesPoint") is not None:
            try:
                set_fields["salesPoint"] = int(it["salesPoint"])
            except Exception:
                pass

        cat_id = category_ids[0] if len(category_ids) == 1 else it.get("categoryId")
        if it.get("bestsellerRank") is not None:
            set_fields["bestseller"] = {
                "rank": int(it["bestsellerRank"]),
                "categoryId": int(cat_id) if cat_id else None,
                "capturedAt": now,
            }

        ops.append(UpdateOne(
            {"uniqueKey": key},           # ê¸°ì¡´ ë¬¸ì„œ ê¸°ì¤€(ISBN ë˜ëŠ” link)
            {"$set": set_fields},
            upsert=False                  # ë©”íƒ€ ì „ìš© ì—…ë°ì´íŠ¸: ì—†ìœ¼ë©´ ë§Œë“¤ì§€ ì•ŠìŒ
        ))

    if ops:
        db[BOOKS_COLLECTION].bulk_write(ops, ordered=False)
