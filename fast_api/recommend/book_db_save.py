# -*- coding: utf-8 -*-
# FastAPI + Aladin API â†’ (í•„í„°/ì¤‘ë³µì œê±°) â†’ [ì˜µì…˜] MongoDB ì €ì¥(save=true)
# ì‹¤í–‰:
#   pip install fastapi uvicorn pymongo requests
#   uvicorn main:app --reload --host 0.0.0.0 --port 8000
# ì‚¬ìš© ì˜ˆ:
#   GET /books?pages=2&per_page=50&save=true
#   GET /_ping_write  â† Mongo ì“°ê¸° í™•ì¸ìš©
#   POST /_repair_isbn_index  â† isbn13:null ì •ë¦¬ + ì¸ë±ìŠ¤ ì¬ìƒì„±

import requests
from typing import List, Dict, Optional
from fastapi import FastAPI, Query, HTTPException
from datetime import datetime
from xml.etree import ElementTree as ET

from pymongo import MongoClient, UpdateOne, ASCENDING, TEXT
from pymongo.errors import DuplicateKeyError, BulkWriteError

# =========================
# ğŸ”§ ê³ ì • ì„¤ì •(ì—¬ê¸°ë§Œ ìˆ˜ì •)
# =========================
# 1) ì•Œë¼ë”˜ TTB í‚¤
TTBKEY = "ttb2win0min1217001"  # â† ë„¤ í‚¤ë¡œ êµì²´

# 2) MongoDB ì—°ê²° ë° DB/ì»¬ë ‰ì…˜ ì´ë¦„
MONGODB_URI = "mongodb+srv://Dgict_TeamB:team1234@cluster0.5d0uual.mongodb.net/"
MONGODB_DB  = "test123"

# ğŸ‘‰ ì»¬ë ‰ì…˜ ì´ë¦„ì€ ì—¬ê¸°ì„œ í•œ ë²ˆë§Œ ì§€ì •
BOOKS_COLLECTION      = "aladin_books"
CATEGORIES_COLLECTION = "aladin_categories"

# 3) ê¸°ë³¸ ê²½ì œ ì¹´í…Œê³ ë¦¬(í•„ìš”ì‹œ ìˆ˜ì •/ì¶”ê°€)
DEFAULT_ECON_CATEGORY_IDS = [
    170, 3057, 3059, 3061, 3062, 3063, 8586, 3065, 3140, 8587,
    2172, 3103, 2173, 2841, 8593, 2747, 3123, 3069, 2028, 853,
    852, 854, 261, 268, 273, 1632, 55058, 2169, 263, 172, 141092,
    11502, 175, 11501, 2225, 174, 177, 3048, 32288, 180, 249,
    3049, 3104, 2408, 3110, 11503, 6189
]

# 4) ì•Œë¼ë”˜ API ê¸°ë³¸
ALADIN_URL = "http://www.aladin.co.kr/ttb/api/ItemList.aspx"

app = FastAPI(title="Aladin Save-First API (Mongo)")

# =========================
# ê³µí†µ ìœ í‹¸
# =========================
def normalize_category_ids(raw_ids: Optional[List[str]]) -> List[int]:
    """'3065,3057' ì´ë“  ['3065','3057'] ì´ë“  ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ê·œí™”"""
    if not raw_ids:
        return []
    out: List[int] = []
    for v in raw_ids:
        if v is None:
            continue
        for part in str(v).split(","):
            p = part.strip()
            if p.isdigit():
                out.append(int(p))
    return list(dict.fromkeys(out))  # ì¤‘ë³µ ì œê±°(ìˆœì„œ ë³´ì¡´)

# =========================
# Mongo ìœ í‹¸
# =========================
def get_db():
    client = MongoClient(MONGODB_URI)
    return client[MONGODB_DB]

def drop_index_if_exists(coll, name: str):
    try:
        coll.drop_index(name)
        print(f"[ensure_indexes] dropped index: {name}")
    except Exception:
        # ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ë¬´ì‹œ
        pass

def ensure_indexes(db):
    # ì¹´í…Œê³ ë¦¬
    db[CATEGORIES_COLLECTION].create_index([("id", ASCENDING)], unique=True, name="uk_category_id")
    db[CATEGORIES_COLLECTION].create_index([("name", ASCENDING)], name="idx_category_name")

    # ë„ì„œ
    db[BOOKS_COLLECTION].create_index([("uniqueKey", ASCENDING)], unique=True, sparse=True, name="uk_uniquekey")

    # (í•µì‹¬) isbn13 ì¸ë±ìŠ¤: partialFilterExpression ì“°ì§€ ì•Šê³  unique + sparse ì‚¬ìš©
    #   â†’ isbn13 í•„ë“œê°€ "ì¡´ì¬í•˜ì§€ ì•ŠëŠ”" ë¬¸ì„œëŠ” ì¸ë±ìŠ¤ ëŒ€ìƒì—ì„œ ì œì™¸ë¨.
    # ë¨¼ì € ì´ì „ì— ì‹¤íŒ¨/ê¸°ì¡´ì— ìˆë˜ ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ì •ë¦¬
    drop_index_if_exists(db[BOOKS_COLLECTION], "uk_isbn13")
    db[BOOKS_COLLECTION].create_index([("isbn13", ASCENDING)], unique=True, sparse=True, name="uk_isbn13")

    db[BOOKS_COLLECTION].create_index([("link", ASCENDING)], unique=True, sparse=True, name="uk_link")
    db[BOOKS_COLLECTION].create_index([("categoryId", ASCENDING), ("pubDate", ASCENDING)], name="idx_cat_pubDate")
    db[BOOKS_COLLECTION].create_index([("pubDate", ASCENDING)], name="idx_pubDate")
    db[BOOKS_COLLECTION].create_index([("title", TEXT), ("author", TEXT)], name="txt_title_author")

# =========================
# ìœ ì§€ë³´ìˆ˜/ì§„ë‹¨ ì—”ë“œí¬ì¸íŠ¸
# =========================
@app.get("/_ping_write")
def ping_write():
    """MongoDB ì—°ê²°/ì“°ê¸° í™•ì¸ìš© ê°„ë‹¨ í…ŒìŠ¤íŠ¸"""
    try:
        db = get_db()
        ensure_indexes(db)
        now = datetime.utcnow()
        doc = {
            "uniqueKey": f"_ping_{now.timestamp()}",
            "title": "PING",
            "author": "SYSTEM",
            "pubDate": now,
            "categoryId": 0,
            "source": "TEST",
            "ingestedAt": now,
            "updatedAt": now,
        }
        db[BOOKS_COLLECTION].insert_one(doc)
        return {"ok": True, "collection": BOOKS_COLLECTION, "db": MONGODB_DB}
    except Exception as e:
        return {"ok": False, "error": str(e)}

@app.post("/_repair_isbn_index")
def repair_isbn_index():
    """
    1) isbn13:null ë¬¸ì„œ ì •ë¦¬(unset)
    2) uk_isbn13 ì¸ë±ìŠ¤ ì¬ìƒì„±(sparse unique)
    """
    db = get_db()
    # 1) isbn13:null â†’ í•„ë“œ ì œê±°
    res = db[BOOKS_COLLECTION].update_many({"isbn13": None}, {"$unset": {"isbn13": ""}})
    # 2) ì¸ë±ìŠ¤ ì¬ìƒì„±
    drop_index_if_exists(db[BOOKS_COLLECTION], "uk_isbn13")
    db[BOOKS_COLLECTION].create_index([("isbn13", ASCENDING)], unique=True, sparse=True, name="uk_isbn13")
    return {"ok": True, "unset_null_count": res.modified_count, "index": "uk_isbn13 (unique,sparse)"}

# =========================
# Aladin í˜¸ì¶œ & íŒŒì‹±
# =========================
def fetch_xml(cid: int, start: int, max_results: int) -> str:
    if not TTBKEY:
        raise HTTPException(500, "ALADIN_TTBKEY ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    params = {
        "ttbkey": TTBKEY,
        "QueryType": "Bestseller",
        "MaxResults": max_results,
        "start": start,
        "SearchTarget": "Book",
        "CategoryId": cid,
        "output": "xml",
        "Version": 20131101,
    }
    r = requests.get(ALADIN_URL, params=params, timeout=10)
    r.raise_for_status()
    return r.text

def parse_xml(xml_text: str) -> List[Dict]:
    try:
        root = ET.fromstring(xml_text)
    except ET.ParseError:
        return []
    for el in root.iter():
        if isinstance(el.tag, str) and '}' in el.tag:
            el.tag = el.tag.split('}', 1)[1]
    out = []
    for it in root.iter("item"):
        def get(tag):
            n = it.find(tag)
            return (n.text or "").strip() if n is not None else ""
        out.append({
            "title":  get("title"),
            "author": get("author"),
            "pubDate": get("pubDate")[:10],  # YYYY-MM-DD
            "link":   get("link"),
            "cover":  get("cover"),
            "isbn13": get("isbn13"),
        })
    return out

# =========================
# Mongo ì €ì¥(ì—…ì„œíŠ¸) + ì§„ë‹¨ ë¡œê·¸
# =========================
def save_to_mongo(items: List[Dict], category_ids: List[int]):
    print(f"[save_to_mongo] incoming items: {len(items)}  cats={category_ids}")
    db = get_db()
    ensure_indexes(db)

    # ì¹´í…Œê³ ë¦¬ upsert
    for cid in set(category_ids):
        db[CATEGORIES_COLLECTION].update_one(
            {"id": int(cid)},
            {"$set": {"id": int(cid), "name": "ECON"},
             "$setOnInsert": {"createdAt": datetime.utcnow()}},
            upsert=True,
        )

    ops = []
    now = datetime.utcnow()
    for it in items:
        raw_isbn = (it.get("isbn13") or "").strip()
        isbn = raw_isbn if raw_isbn else None
        link = (it.get("link") or "").strip() or None
        unique_key = isbn or link
        if not unique_key:
            # isbnë„ linkë„ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            continue

        pub_dt = None
        if it.get("pubDate"):
            try:
                pub_dt = datetime.strptime(it["pubDate"], "%Y-%m-%d")
            except Exception:
                pub_dt = None

        # category_idsê°€ ë‹¨ì¼ì´ë©´ ê·¸ ê°’ì„ categoryIdë¡œ, ë³µìˆ˜ë©´ ì•„ì´í…œì— ì‹¬ì–´ë†“ì€ ê°’ ì‚¬ìš©
        cat_id = category_ids[0] if len(category_ids) == 1 else it.get("categoryId")

        # $set í•„ë“œ êµ¬ì„± (isbn13ì´ Noneì´ë©´ ì•„ì˜ˆ ì €ì¥í•˜ì§€ ì•ŠìŒ)
        set_fields = {
            "title": it.get("title") or "",
            "author": it.get("author") or "",
            "cover": it.get("cover") or None,
            "pubDate": pub_dt,
            "categoryId": int(cat_id) if cat_id else None,
            "source": "ALADIN",
            "link": link,
            "updatedAt": now,
        }
        if isbn is not None:
            set_fields["isbn13"] = isbn

        update_doc = {"$set": set_fields, "$setOnInsert": {"ingestedAt": now, "uniqueKey": unique_key}}
        # í˜¹ì‹œ ì´ì „ì— isbn13:null ì´ ì €ì¥ëœ ì ì´ ìˆë‹¤ë©´ ì œê±°
        if isbn is None:
            update_doc["$unset"] = {"isbn13": ""}

        ops.append(UpdateOne({"uniqueKey": unique_key}, update_doc, upsert=True))

    if not ops:
        print("[save_to_mongo] no ops to write (missing isbn13/link?)")
        return

    try:
        result = db[BOOKS_COLLECTION].bulk_write(ops, ordered=False)
        print(f"[save_to_mongo] bulk_write OK. upserted={result.upserted_count} matched={result.matched_count} modified={result.modified_count}")
    except DuplicateKeyError as e:
        print(f"[save_to_mongo] DuplicateKeyError: {e}")
    except BulkWriteError as bwe:
        # ì¼ë¶€ ë¬¸ì„œ ì‹¤íŒ¨ ì‹œì—ë„ ì„±ê³µë¶„ì€ ë°˜ì˜ë¨. ì„¸ë¶€ ì‚¬ìœ ë§Œ ë¡œê·¸
        print(f"[save_to_mongo] BulkWriteError: {bwe.details}")
    except Exception as e:
        print(f"[save_to_mongo] bulk_write error: {e}")
        raise

# =========================
# API ì—”ë“œí¬ì¸íŠ¸
# =========================
@app.get("/books")
def books(
    # ì½¤ë§ˆ/ë°˜ë³µ í—ˆìš©
    category_ids: Optional[List[str]] = Query(
        None, description="ì˜ˆ: 3065,3057 ë˜ëŠ” category_ids=3065&category_ids=3057"
    ),
    start: int  = Query(1, ge=1),
    pages: int  = Query(1, ge=1, le=5),
    per_page: int = Query(20, ge=1, le=50),
    since: Optional[str] = None,  # YYYY-MM-DD
    save: bool = Query(False, description="trueë©´ MongoDBì— ì €ì¥ í›„ ë°˜í™˜"),
):
    # 1) ì¹´í…Œê³ ë¦¬ ì •ê·œí™”
    ids = normalize_category_ids(category_ids)
    if not ids:
        ids = DEFAULT_ECON_CATEGORY_IDS
    if not ids:
        raise HTTPException(400, "category_idsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    # 2) ìˆ˜ì§‘
    collected: List[Dict] = []
    for cid in ids:
        s = start
        for _ in range(pages):
            try:
                xml = fetch_xml(cid, s, per_page)
                parsed = parse_xml(xml)
                # í˜¸ì¶œë‹¹ì‹œ cidë¥¼ ê° ì•„ì´í…œì— ì‹¬ì–´ë‘ 
                for it in parsed:
                    it.setdefault("categoryId", cid)
                collected.extend(parsed)
            except requests.RequestException as e:
                print(f"[books] request error (cid={cid}, page={s}): {e}")
            s += 1

    print(f"[books] collected={len(collected)} before dedup")

    # 3) ì¤‘ë³µ ì œê±° (isbn13 ìš°ì„ , ì—†ìœ¼ë©´ link)
    seen = set()
    dedup = []
    for it in collected:
        key = it.get("isbn13") or it.get("link")
        if not key:
            continue
        if key in seen:
            continue
        seen.add(key)
        dedup.append(it)

    print(f"[books] after dedup={len(dedup)}")

    # 4) since í•„í„° (ì„ íƒ)
    if since:
        try:
            since_dt = datetime.strptime(since[:10], "%Y-%m-%d")
            filtered = []
            for it in dedup:
                pub = (it.get("pubDate") or "")[:10]
                try:
                    if datetime.strptime(pub, "%Y-%m-%d") >= since_dt:
                        filtered.append(it)
                except Exception:
                    filtered.append(it)
            dedup = filtered
            print(f"[books] after since filter={len(dedup)} (since={since})")
        except ValueError:
            print(f"[books] invalid since format, ignored: {since}")

    # 5) save=trueì´ë©´ Mongoì— ì—…ì„œíŠ¸
    if save:
        try:
            save_to_mongo(dedup, ids)
        except Exception as e:
            raise HTTPException(500, f"Mongo ì €ì¥ ì‹¤íŒ¨: {e}")

    return {"count": len(dedup), "items": dedup, "saved": bool(save)}
