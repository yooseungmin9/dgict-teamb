# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ + ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ + ê°ì •ë¶„ì„ + Mongo ì €ì¥ (ë‹¨ì¼ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸)
"""

import os
import re
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Tuple
from urllib.parse import urlparse, parse_qs

import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient, ASCENDING, errors
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# -------------------
# í™˜ê²½/ìƒìˆ˜
# -------------------
MONGODB_URI = os.getenv(
    "MONGODB_URI",
    "mongodb+srv://Dgict_TeamB:team1234@cluster0.5d0uual.mongodb.net/?retryWrites=true&w=majority",
)
DB_NAME = os.getenv("MONGODB_DB", "test123")
COLL_NAME = os.getenv("MONGODB_COLL", "news")
USER_AGENT = "Mozilla/5.0"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
)

# -------------------
# ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ
# -------------------
CATEGORY_KEYWORDS: Dict[str, List[str]] = {
    "ì¦ê¶Œ": ["ì£¼ì‹", "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì¦ê¶Œ", "ìƒì¥", "ê±°ë˜ì†Œ", "ì¦ì‹œ", "ê³µëª¨ì£¼", "IPO", "PER", "ETF"],
    "ê¸ˆìœµ": ["ì€í–‰", "ê¸ˆë¦¬", "ëŒ€ì¶œ", "ì˜ˆê¸ˆ", "ì ê¸ˆ", "ë³´í—˜", "ì¹´ë“œ", "ê¸ˆìœµ", "Fed", "ì—°ì¤€", "ê¸ˆí†µìœ„", "ì±„ê¶Œ"],
    "ë¶€ë™ì‚°": ["ë¶€ë™ì‚°", "ì•„íŒŒíŠ¸", "ì£¼íƒ", "ë¶„ì–‘", "ì „ì„¸", "ë§¤ë§¤", "ì²­ì•½", "ì¬ê±´ì¶•", "ë¶€ë™ì‚°ì‹œì¥"],
    "ì‚°ì—…": ["ì‚°ì—…", "ì œì¡°", "ì¬ê³„", "ê¸°ì—…", "ìë™ì°¨", "ë°˜ë„ì²´", "ì² ê°•", "ë°”ì´ì˜¤", "IT", "ê³µì¥", "ì—ë„ˆì§€"],
    "ê¸€ë¡œë²Œê²½ì œ": ["ê¸€ë¡œë²Œ", "ì„¸ê³„", "í•´ì™¸", "êµ­ì œ", "ë¯¸êµ­", "ì¤‘êµ­", "ì¼ë³¸", "ë‹¬ëŸ¬", "ìœ„ì•ˆ", "ìœ ëŸ½"],
    "ì¼ë°˜": [],
}

POSITIVE_WORDS = [
    "í˜¸í™©","ìƒìŠ¹","ì„±ì¥","í˜¸ì¬","ê°•ì„¸","ì´ìµ","í‘ì","ê°œì„ ","íšŒë³µ","í™•ëŒ€","ì‹ ê¸°ë¡","ì•ˆì •","ê¸ì •","ë‹¬ì„±","ëŒíŒŒ",
    "ê¸‰ë“±","í™œì„±í™”","ê°•í™”","í˜¸ì‘","í˜¸ì¡°","ìˆ˜ìµ","ì‹¤ì ê°œì„ ","ìµœê³ ì¹˜","ì•ˆì •ì„¸","í˜¸ì „","ê°€ì†í™”","ì•½ì§„","í™•ë³´","í˜ì‹ ","ì„ ë„","ì§„ì¶œ",
    "ì§€ì›","íˆ¬ì","ê³ ìš©í™•ëŒ€","í™œí™©","ë„ì•½","ì„±ê³µ","ì„±ê³¼","ì „ë§ë°ìŒ"
]
NEGATIVE_WORDS = [
    "ë¶ˆí™©","í•˜ë½","ì ì","ìœ„ê¸°","ì•½ì„¸","ì†ì‹¤","ì¹¨ì²´","ì•…í™”","ìœ„ì¶•","ê¸‰ë½","ë¶•ê´´","ë¶ˆì•ˆ","ë¶€ì •","íŒŒì‚°","ë¶€ë„",
    "ê¸‰ê°","ê²½ê³ ","ìœ„í˜‘","ì¶”ë½","ë§ˆì´ë„ˆìŠ¤","ì ìí­","ì €ì¡°","ê°ì†Œì„¸","ê¸‰ë“±ë½","ìœ„í—˜","ë¶ˆí™•ì‹¤","ë¶€ì§„","ê¸‰ì¦ì„¸","ì•½í™”","ì •ì²´","ì†Œë©¸",
    "í•´ê³ ","êµ¬ì¡°ì¡°ì •","íì—…","ë…¼ë€","ê°ˆë“±","ì¹¨ì²´ê¸°","ì‹¤íŒ¨","ì°¨ì§ˆ"
]

# -------------------
# HTTP ì„¸ì…˜ (ì¬ì‹œë„/íƒ€ì„ì•„ì›ƒ)
# -------------------
def make_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({"User-Agent": USER_AGENT, "Accept-Language": "ko-KR,ko;q=0.9"})
    retry = Retry(
        total=3,
        backoff_factor=0.4,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=frozenset(["GET"]),
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retry)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s

# -------------------
# Mongo
# -------------------
def get_collection():
    client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=5000)
    # ì—°ê²° ì²´í¬ (DNS, ì¸ì¦, TLS ë“± ë¬¸ì œ ì¡°ê¸° í¬ì°©)
    client.admin.command("ping")
    db = client[DB_NAME]
    col = db[COLL_NAME]
    # ì¤‘ë³µë°©ì§€ ë° ì¡°íšŒ ì¸ë±ìŠ¤
    col.create_index([("link", ASCENDING)], name="uniq_link", unique=True)
    col.create_index([("category", ASCENDING), ("news_date", ASCENDING)], name="q_cat_date")
    col.create_index([("news_date", ASCENDING)], name="q_date")
    return col

# -------------------
# í¬ë¡¤ë§ ìœ í‹¸
# -------------------
def build_list_url(date: str, page: int) -> str:
    base = "https://news.naver.com/main/list.naver"
    # 101 = ê²½ì œ
    return f"{base}?mode=LSD&mid=sec&sid1=101&date={date}&page={page}"

def extract_links(html: str) -> List[Tuple[str, str]]:
    soup = BeautifulSoup(html, "html.parser")
    anchors = soup.select("ul.type06_headline li dt a, ul.type06 li dt a")
    out: List[Tuple[str, str]] = []
    for a in anchors:
        href = a.get("href", "")
        if "/article/" in href:
            out.append((a.get_text(strip=True), href))
    return out

def fetch_article(session: requests.Session, link: str) -> str:
    r = session.get(link, timeout=10)
    r.raise_for_status()
    s = BeautifulSoup(r.text, "html.parser")
    node = s.select_one("article#dic_area")
    return node.get_text(" ", strip=True) if node else "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

# -------------------
# NLP ìœ í‹¸
# -------------------
def classify_category(text: str) -> str:
    for cat, kws in CATEGORY_KEYWORDS.items():
        for kw in kws:
            if kw and kw in text:
                return cat
    return "ì¼ë°˜"

def analyze_sentiment(text: str) -> str:
    pos = sum(1 for w in POSITIVE_WORDS if w in text)
    neg = sum(1 for w in NEGATIVE_WORDS if w in text)
    return "ê¸ì •" if pos > neg else "ë¶€ì •" if neg > pos else "ì¤‘ë¦½"

# -------------------
# ë³¸ë¬¸: ìˆ˜ì§‘â†’ë¶„ì„â†’ì €ì¥
# -------------------
def crawl_and_save(days: int = 5, limit_per_day: int = 50, sleep_sec: float = 0.2) -> None:
    session = make_session()
    col = get_collection()

    for i in range(days):
        date = (datetime.now() - timedelta(days=i)).strftime("%Y%m%d")
        logging.info("ğŸ“… %s | ê²½ì œ ì „ì²´", date)

        page, inserted, skipped = 1, 0, 0

        while inserted < limit_per_day:
            url = build_list_url(date, page)
            try:
                res = session.get(url, timeout=10)
                res.raise_for_status()
            except Exception as e:
                logging.warning("ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨: %s (page=%d) -> ë‹¤ìŒ í˜ì´ì§€ ì‹œë„", e, page)
                page += 1
                continue

            links = extract_links(res.text)
            if not links:
                break

            for title, link in links:
                # ì¤‘ë³µ ì²´í¬
                if col.find_one({"link": link}):
                    skipped += 1
                    continue

                try:
                    content = fetch_article(session, link)
                except Exception as e:
                    content = f"ë³¸ë¬¸ ì—ëŸ¬: {e}"

                category = classify_category(f"{title} {content}")
                sentiment = analyze_sentiment(content)

                # news_dateëŠ” ëª©ë¡ ê¸°ì¤€(YYYYMMDD). í•„ìš”ì‹œ ë³¸ë¬¸ì—ì„œ ë‚ ì§œ íŒŒì‹± ë¡œì§ ì¶”ê°€ ê°€ëŠ¥.
                doc = {
                    "title": title,
                    "link": link,
                    "content": content,
                    "category": category,
                    "sentiment": sentiment,
                    "news_date": date,
                    "saved_at": datetime.now().isoformat(timespec="seconds"),
                }

                try:
                    col.insert_one(doc)
                    inserted += 1
                except errors.DuplicateKeyError:
                    skipped += 1
                except Exception as e:
                    logging.error("DB ì €ì¥ ì‹¤íŒ¨: %s | %s", e, link)

                if inserted >= limit_per_day:
                    break

                time.sleep(sleep_sec)

            page += 1

        logging.info("   â–¶ ì €ì¥ ê²°ê³¼ | insert=%d, skip=%d", inserted, skipped)

    logging.info("âœ… ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ")

# -------------------
# ì‹¤í–‰ë¶€
# -------------------
if __name__ == "__main__":
    # í•„ìš”ì— ë”°ë¼ íŒŒë¼ë¯¸í„° ì¡°ì •
    crawl_and_save(days=5, limit_per_day=50, sleep_sec=0.2)
